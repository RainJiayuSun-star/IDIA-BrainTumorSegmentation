{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9O2FMEC9OeU"
   },
   "source": [
    "# Overview\n",
    "**BRAIN SEGMENTATION INFERENCE**\n",
    "This Jupyter notebook is designed to run inference on a brain MRI scan using a pre-trained segmentation model. It downloads a model from the cloud (requiring AWS access keys to be set up properly) and then runs inference on a single scan. The output is a nifti file with the segmentation labels. You may wish to then load it in a program such as Slicer to view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjoOMwOTNJTd"
   },
   "source": [
    "# Setup\n",
    "First, set up a python environment with the necessary packages.\n",
    "\n",
    "For instance, in VSCode or Cursor, press Ctrl+Shift+P and type \"Python: Create Environment\" and follow the prompts.\n",
    "\n",
    "Or, on the command line, run:\n",
    "```bash\n",
    "python -m venv brain_segmentation_inference_env\n",
    "```\n",
    "--or--\n",
    "```bash\n",
    "python3 -m venv brain_segmentation_inference_env\n",
    "```\n",
    "and then either \n",
    "```bash\n",
    "brain_segmentation_inference_env\\Scripts\\activate\n",
    "```\n",
    "on Windows, or\n",
    "```bash\n",
    "source brain_segmentation_inference_env/bin/activate\n",
    "```\n",
    "on macOS and Linux.\n",
    "\n",
    "Then, install the necessary packages:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After all that, this cell should run without error and import all the necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (2.5.5)\n",
      "Requirement already satisfied: wandb in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (0.22.0)\n",
      "Requirement already satisfied: omegaconf in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: hydra-core in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: einops in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (0.8.1)\n",
      "Requirement already satisfied: pillow in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (11.2.1)\n",
      "Requirement already satisfied: boto3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 8)) (1.40.36)\n",
      "Requirement already satisfied: monai[nibabel] in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.24 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (2.2.6)\n",
      "Requirement already satisfied: torch<2.7.0,>=2.4.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: nibabel in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (5.3.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.38.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/rainsun/miniconda3/lib/python3.13/site-packages (from omegaconf->-r ../requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.36 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (1.40.36)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (2.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 3)) (4.0.12)\n",
      "Requirement already satisfied: setuptools in /home/rainsun/miniconda3/lib/python3.13/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning->-r ../requirements.txt (line 2)) (78.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: filelock in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: networkx in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 3)) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from jinja2->torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "cuda devices (count) .... 1\n"
     ]
    }
   ],
   "source": [
    "#Check GPU availability\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print('cuda devices (count) ....',torch.cuda.device_count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 23 03:10:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.03              Driver Version: 576.28         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8              2W /  115W |     181MiB /   8188MiB |     17%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:10:49,886 - brain_segmentation - INFO - üöÄ Setting up logging...\n",
      "2025-09-23 03:10:49,887 - brain_segmentation - DEBUG - üîß Current logging level: 10\n",
      "2025-09-23 03:10:49,888 - brain_segmentation - INFO - ‚úÖ Logging setup complete.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Create a named logger\n",
    "logger = logging.getLogger('brain_segmentation')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a console handler and set its level\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the formatter to the console handler\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Turn off logs from all other loggers\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name != 'brain_segmentation':\n",
    "        logging.getLogger(name).setLevel(logging.CRITICAL)\n",
    "\n",
    "logger.info('üöÄ Setting up logging...')\n",
    "logger.debug(f'üîß Current logging level: {logger.getEffectiveLevel()}')\n",
    "logger.info('‚úÖ Logging setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SdAAZeqmPNsv",
    "outputId": "2706f96f-1b8d-4f0d-9f3e-b0f3c8dbaa2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:10:49,894 - brain_segmentation - INFO - üì¶ Importing outside packages...\n",
      "/home/rainsun/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-23 03:10:51,793 - brain_segmentation - INFO - ‚úÖ Outside packages imported.\n",
      "2025-09-23 03:10:51,793 - brain_segmentation - INFO - üì¶ Importing local packages...\n",
      "2025-09-23 03:10:51,802 - brain_segmentation - INFO - ‚úÖ Local packages imported.\n",
      "2025-09-23 03:10:51,803 - brain_segmentation - INFO - üì¶ Setting up device...\n",
      "2025-09-23 03:10:51,891 - brain_segmentation - INFO - ‚úÖ Device set up.\n",
      "2025-09-23 03:10:51,891 - brain_segmentation - DEBUG - üñ•Ô∏è device = device(type='cuda', index=0)\n",
      "2025-09-23 03:10:51,894 - brain_segmentation - INFO - üìÇ Output directory set to: data/output_my\n"
     ]
    }
   ],
   "source": [
    "logger.info('üì¶ Importing outside packages...')\n",
    "import os, sys\n",
    "import torch\n",
    "import glob\n",
    "import json, yaml\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import botocore\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from smart_open import open\n",
    "from monai.transforms import AsDiscrete, Activations\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "logger.info('‚úÖ Outside packages imported.')\n",
    "\n",
    "logger.info('üì¶ Importing local packages...')\n",
    "from core_common import get_loader_val, datafold_read\n",
    "import model\n",
    "logger.info('‚úÖ Local packages imported.')\n",
    "\n",
    "logger.info('üì¶ Setting up device...')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('‚úÖ Device set up.')\n",
    "logger.debug(f'üñ•Ô∏è {device = }')\n",
    "\n",
    "# make output directory\n",
    "output_dir = \"data/output_my\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"üìÇ Output directory set to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OeN_BW_NJTj"
   },
   "source": [
    "# Model\n",
    "This part loads the model from the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX5dkAr0NJTl"
   },
   "source": [
    "## Download\n",
    "This part downloads the model from the cloud.\n",
    "\n",
    "You'll need to have an AWS profile named 'theta-model-downloader'. You can create this profile by running this command in your terminal:\n",
    "```bash\n",
    "aws configure --profile theta-model-downloader\n",
    "```\n",
    "and entering your AWS credentials.\n",
    "\n",
    "If you do not have the AWS command line tools installed, you can install them by following the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cli/2.30.1 Python/3.13.7 Linux/5.15.167.4-microsoft-standard-WSL2 exe/x86_64.ubuntu.24\n"
     ]
    }
   ],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting the input data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter if you want to run inference on 'time1' or 'time2' time1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User choose to run inference on time1\n",
      "‚ùå Missing files for UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_flair.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t1ce.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t1.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t2.nii.gz\n",
      "\n",
      "‚úÖ Total valid cases: 298\n",
      "üß™ Sample valid case IDs: ['100001', '100002', '100003', '100004', '100005']\n"
     ]
    }
   ],
   "source": [
    "# choose to run inference on time1 or time2\n",
    "time_num = input(\"Please enter if you want to run inference on 'time1' or 'time2'\")\n",
    "print(\"User choose to run inference on \" + time_num)\n",
    "'''main_path=\"/app/train/tumor_seg/tumor_seg/data/Preprocessed\"\n",
    "\n",
    "cases_list=glob.glob(os.path.join(main_path, \"*\"))\n",
    "cases_list=sorted(cases_list)\n",
    "\n",
    "new_ids=[]\n",
    "for paths_files in cases_list:\n",
    "    case_id=Path(paths_files)\n",
    "    case_id=case_id.parts[-3]\n",
    "    print(case_id)\n",
    "    dir_name=os.path.dirname(paths_files)\n",
    "    print(dir_name)\n",
    "    files = [dir_name+'/'+case_id+'_time1_flair.nii.gz', \n",
    "             dir_name+'/'+case_id + '_time1_t1ce.nii.gz',\n",
    "             dir_name+'/'+case_id + '_time1_t1.nii.gz', \n",
    "             dir_name+'/'+case_id + '_time1_t2.nii.gz']\n",
    "    \n",
    "    all_exist = all([os.path.exists(file) for file in files])\n",
    "    if all_exist:\n",
    "        new_ids.append(case_id)\n",
    "\n",
    "len(new_ids)'''\n",
    "main_path = \"/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed\"\n",
    "cases_list = sorted(glob.glob(os.path.join(main_path, \"*\")))\n",
    "\n",
    "new_ids = []\n",
    "\n",
    "for case_path in cases_list:\n",
    "    case_id = Path(case_path).name  # ‚úÖ correct way to extract the folder name (e.g., \"100001\")\n",
    "    \n",
    "    '''\n",
    "    files = [\n",
    "        os.path.join(case_path, f\"{case_id}_time1_flair.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t1ce.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t1.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t2.nii.gz\"),\n",
    "    ]\n",
    "    '''\n",
    "    files = [\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_flair.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t1ce.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t1.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t2.nii.gz\"),\n",
    "    ]\n",
    "\n",
    "    all_exist = all(os.path.exists(f) for f in files)\n",
    "    if all_exist:\n",
    "        new_ids.append(case_id)\n",
    "    else:\n",
    "        print(f\"‚ùå Missing files for {case_id}\")\n",
    "        for f in files:\n",
    "            if not os.path.exists(f):\n",
    "                print(f\"   - {f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total valid cases: {len(new_ids)}\")\n",
    "print(\"üß™ Sample valid case IDs:\", new_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnew_ids=[\"0171\", \"0178\", \"0179\" ,\"0190\", \"0196\", \"0198\", \"0199\",\"0245\",\\n         \"0281\",\"0296\", \"0325\", \"0338\", \"355\"]\\nnew_ids=[\"0190\"]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "new_ids=[\"0171\", \"0178\", \"0179\" ,\"0190\", \"0196\", \"0198\", \"0199\",\"0245\",\n",
    "         \"0281\",\"0296\", \"0325\", \"0338\", \"355\"]\n",
    "new_ids=[\"0190\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'training': []\n",
    "}\n",
    "\n",
    "for id_num in new_ids:\n",
    "    new_entry = {\n",
    "        'fold': 1,\n",
    "        'label': f'{id_num}/{id_num}_{time_num}_seg.nii.gz',\n",
    "        'image': [\n",
    "            f'{id_num}/{id_num}_{time_num}_flair.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t1ce.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t1.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t2.nii.gz'\n",
    "        ]\n",
    "    }\n",
    "    # Append the new entry to the 'training' list\n",
    "    data['training'].append(new_entry)\n",
    "    \n",
    "with open('./data/multi_example2.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fold': 1,\n",
       "  'label': '100001/100001_time1_seg.nii.gz',\n",
       "  'image': ['100001/100001_time1_flair.nii.gz',\n",
       "   '100001/100001_time1_t1ce.nii.gz',\n",
       "   '100001/100001_time1_t1.nii.gz',\n",
       "   '100001/100001_time1_t2.nii.gz']}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path=\"./data/multi_example2.json\"\n",
    "with open(files_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# check first instance JSON data\n",
    "data[\"training\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:00,336 - brain_segmentation - INFO - üìÇ Loading configuration file...\n",
      "2025-09-23 03:11:00,336 - brain_segmentation - INFO - üîì Opening configuration file...\n",
      "2025-09-23 03:11:00,340 - brain_segmentation - INFO - üìñ Reading YAML content...\n",
      "2025-09-23 03:11:00,342 - brain_segmentation - DEBUG - üîß inference_cfg = {'input': {'compute_dice': False, 'mode': 'multi', 'multi_scan': {'scan_list': 'data/multi_example2.json'}, 'single_scan': {'ground_truth': '100001/100001_time1_seg.nii.gz', 'flair': '100001/100001_time1_flair.nii.gz', 't1c': '100001/100001_time1_t1ce.nii.gz', 't1': '100001/100001_time1_t1.nii.gz', 't2': '100001/100001_time1_t2.nii.gz'}, 'data_dir': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed'}, 'output': {'file_path': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/out_seg.nii.gz'}, 'model': {'bucket': 'theta-trained-models', 'key': 'model.ckpt', 'region': 'us-east-1'}}\n",
      "2025-09-23 03:11:00,343 - brain_segmentation - INFO - ‚úÖ Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info('üìÇ Loading configuration file...')\n",
    "config_pth = '../config/example_config.yaml'\n",
    "\n",
    "logger.info('üîì Opening configuration file...')\n",
    "with open(config_pth, 'r') as file:\n",
    "    logger.info('üìñ Reading YAML content...')\n",
    "    inference_cfg = yaml.safe_load(file)\n",
    "logger.debug(f'üîß {inference_cfg = }')\n",
    "logger.info('‚úÖ Configuration loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the terminal\n",
    "#!aws configure --profile theta-model-downloader\n",
    "# enter your credential: (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb1qCskWNJTm",
    "outputId": "a8a36564-66cf-4cf1-c8e9-b7201c6f6b93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:00,351 - brain_segmentation - INFO - üìÅ Loading checkpoint from ../data/model_checkpoints/4-17-17/swinunetr-epoch=159.ckpt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'compute_dice': False,\n",
      "           'data_dir': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed',\n",
      "           'mode': 'multi',\n",
      "           'multi_scan': {'scan_list': 'data/multi_example2.json'},\n",
      "           'single_scan': {'flair': '100001/100001_time1_flair.nii.gz',\n",
      "                           'ground_truth': '100001/100001_time1_seg.nii.gz',\n",
      "                           't1': '100001/100001_time1_t1.nii.gz',\n",
      "                           't1c': '100001/100001_time1_t1ce.nii.gz',\n",
      "                           't2': '100001/100001_time1_t2.nii.gz'}},\n",
      " 'model': {'bucket': 'theta-trained-models',\n",
      "           'key': 'model.ckpt',\n",
      "           'region': 'us-east-1'},\n",
      " 'output': {'file_path': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/out_seg.nii.gz'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,221 - brain_segmentation - INFO - üìê ROI size set to: [224, 224, 96]\n",
      "2025-09-23 03:11:01,280 - brain_segmentation - INFO - üß† Loading model weights with strict=False (to ignore size mismatches)...\n",
      "2025-09-23 03:11:01,311 - brain_segmentation - INFO - ‚úÖ Custom checkpoint loaded and model ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Checkpoint top-level keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "\n",
      "üîç Inspecting state_dict parameter shapes:\n",
      "swinViT.patch_embed.proj.weight                              (24, 4, 2, 2, 2)\n",
      "swinViT.patch_embed.proj.bias                                (24,)\n",
      "swinViT.layers1.0.blocks.0.norm1.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.0.norm1.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.0.attn.relative_position_bias_table (125, 3)\n",
      "swinViT.layers1.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers1.0.blocks.0.attn.qkv.weight                   (72, 24)\n",
      "swinViT.layers1.0.blocks.0.attn.qkv.bias                     (72,)\n",
      "swinViT.layers1.0.blocks.0.attn.proj.weight                  (24, 24)\n",
      "swinViT.layers1.0.blocks.0.attn.proj.bias                    (24,)\n",
      "swinViT.layers1.0.blocks.0.norm2.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.0.norm2.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear1.weight                (96, 24)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear1.bias                  (96,)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear2.weight                (24, 96)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear2.bias                  (24,)\n",
      "swinViT.layers1.0.blocks.1.norm1.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.1.norm1.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.1.attn.relative_position_bias_table (125, 3)\n",
      "swinViT.layers1.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers1.0.blocks.1.attn.qkv.weight                   (72, 24)\n",
      "swinViT.layers1.0.blocks.1.attn.qkv.bias                     (72,)\n",
      "swinViT.layers1.0.blocks.1.attn.proj.weight                  (24, 24)\n",
      "swinViT.layers1.0.blocks.1.attn.proj.bias                    (24,)\n",
      "swinViT.layers1.0.blocks.1.norm2.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.1.norm2.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear1.weight                (96, 24)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear1.bias                  (96,)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear2.weight                (24, 96)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear2.bias                  (24,)\n",
      "swinViT.layers1.0.downsample.reduction.weight                (48, 192)\n",
      "swinViT.layers1.0.downsample.norm.weight                     (192,)\n",
      "swinViT.layers1.0.downsample.norm.bias                       (192,)\n",
      "swinViT.layers2.0.blocks.0.norm1.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.0.norm1.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers2.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers2.0.blocks.0.attn.qkv.weight                   (144, 48)\n",
      "swinViT.layers2.0.blocks.0.attn.qkv.bias                     (144,)\n",
      "swinViT.layers2.0.blocks.0.attn.proj.weight                  (48, 48)\n",
      "swinViT.layers2.0.blocks.0.attn.proj.bias                    (48,)\n",
      "swinViT.layers2.0.blocks.0.norm2.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.0.norm2.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear1.weight                (192, 48)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear1.bias                  (192,)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear2.weight                (48, 192)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear2.bias                  (48,)\n",
      "swinViT.layers2.0.blocks.1.norm1.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.1.norm1.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers2.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers2.0.blocks.1.attn.qkv.weight                   (144, 48)\n",
      "swinViT.layers2.0.blocks.1.attn.qkv.bias                     (144,)\n",
      "swinViT.layers2.0.blocks.1.attn.proj.weight                  (48, 48)\n",
      "swinViT.layers2.0.blocks.1.attn.proj.bias                    (48,)\n",
      "swinViT.layers2.0.blocks.1.norm2.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.1.norm2.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear1.weight                (192, 48)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear1.bias                  (192,)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear2.weight                (48, 192)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear2.bias                  (48,)\n",
      "swinViT.layers2.0.downsample.reduction.weight                (96, 384)\n",
      "swinViT.layers2.0.downsample.norm.weight                     (384,)\n",
      "swinViT.layers2.0.downsample.norm.bias                       (384,)\n",
      "swinViT.layers3.0.blocks.0.norm1.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.0.norm1.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers3.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers3.0.blocks.0.attn.qkv.weight                   (288, 96)\n",
      "swinViT.layers3.0.blocks.0.attn.qkv.bias                     (288,)\n",
      "swinViT.layers3.0.blocks.0.attn.proj.weight                  (96, 96)\n",
      "swinViT.layers3.0.blocks.0.attn.proj.bias                    (96,)\n",
      "swinViT.layers3.0.blocks.0.norm2.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.0.norm2.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear1.weight                (384, 96)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear1.bias                  (384,)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear2.weight                (96, 384)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear2.bias                  (96,)\n",
      "swinViT.layers3.0.blocks.1.norm1.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.1.norm1.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers3.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers3.0.blocks.1.attn.qkv.weight                   (288, 96)\n",
      "swinViT.layers3.0.blocks.1.attn.qkv.bias                     (288,)\n",
      "swinViT.layers3.0.blocks.1.attn.proj.weight                  (96, 96)\n",
      "swinViT.layers3.0.blocks.1.attn.proj.bias                    (96,)\n",
      "swinViT.layers3.0.blocks.1.norm2.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.1.norm2.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear1.weight                (384, 96)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear1.bias                  (384,)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear2.weight                (96, 384)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear2.bias                  (96,)\n",
      "swinViT.layers3.0.downsample.reduction.weight                (192, 768)\n",
      "swinViT.layers3.0.downsample.norm.weight                     (768,)\n",
      "swinViT.layers3.0.downsample.norm.bias                       (768,)\n",
      "swinViT.layers4.0.blocks.0.norm1.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.0.norm1.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers4.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers4.0.blocks.0.attn.qkv.weight                   (576, 192)\n",
      "swinViT.layers4.0.blocks.0.attn.qkv.bias                     (576,)\n",
      "swinViT.layers4.0.blocks.0.attn.proj.weight                  (192, 192)\n",
      "swinViT.layers4.0.blocks.0.attn.proj.bias                    (192,)\n",
      "swinViT.layers4.0.blocks.0.norm2.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.0.norm2.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear1.weight                (768, 192)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear1.bias                  (768,)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear2.weight                (192, 768)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear2.bias                  (192,)\n",
      "swinViT.layers4.0.blocks.1.norm1.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.1.norm1.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers4.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers4.0.blocks.1.attn.qkv.weight                   (576, 192)\n",
      "swinViT.layers4.0.blocks.1.attn.qkv.bias                     (576,)\n",
      "swinViT.layers4.0.blocks.1.attn.proj.weight                  (192, 192)\n",
      "swinViT.layers4.0.blocks.1.attn.proj.bias                    (192,)\n",
      "swinViT.layers4.0.blocks.1.norm2.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.1.norm2.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear1.weight                (768, 192)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear1.bias                  (768,)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear2.weight                (192, 768)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear2.bias                  (192,)\n",
      "swinViT.layers4.0.downsample.reduction.weight                (384, 1536)\n",
      "swinViT.layers4.0.downsample.norm.weight                     (1536,)\n",
      "swinViT.layers4.0.downsample.norm.bias                       (1536,)\n",
      "encoder1.layer.conv1.conv.weight                             (24, 4, 3, 3, 3)\n",
      "encoder1.layer.conv2.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder1.layer.conv3.conv.weight                             (24, 4, 1, 1, 1)\n",
      "encoder2.layer.conv1.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder2.layer.conv2.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder3.layer.conv1.conv.weight                             (48, 48, 3, 3, 3)\n",
      "encoder3.layer.conv2.conv.weight                             (48, 48, 3, 3, 3)\n",
      "encoder4.layer.conv1.conv.weight                             (96, 96, 3, 3, 3)\n",
      "encoder4.layer.conv2.conv.weight                             (96, 96, 3, 3, 3)\n",
      "encoder10.layer.conv1.conv.weight                            (384, 384, 3, 3, 3)\n",
      "encoder10.layer.conv2.conv.weight                            (384, 384, 3, 3, 3)\n",
      "decoder5.transp_conv.conv.weight                             (384, 192, 2, 2, 2)\n",
      "decoder5.conv_block.conv1.conv.weight                        (192, 384, 3, 3, 3)\n",
      "decoder5.conv_block.conv2.conv.weight                        (192, 192, 3, 3, 3)\n",
      "decoder5.conv_block.conv3.conv.weight                        (192, 384, 1, 1, 1)\n",
      "decoder4.transp_conv.conv.weight                             (192, 96, 2, 2, 2)\n",
      "decoder4.conv_block.conv1.conv.weight                        (96, 192, 3, 3, 3)\n",
      "decoder4.conv_block.conv2.conv.weight                        (96, 96, 3, 3, 3)\n",
      "decoder4.conv_block.conv3.conv.weight                        (96, 192, 1, 1, 1)\n",
      "decoder3.transp_conv.conv.weight                             (96, 48, 2, 2, 2)\n",
      "decoder3.conv_block.conv1.conv.weight                        (48, 96, 3, 3, 3)\n",
      "decoder3.conv_block.conv2.conv.weight                        (48, 48, 3, 3, 3)\n",
      "decoder3.conv_block.conv3.conv.weight                        (48, 96, 1, 1, 1)\n",
      "decoder2.transp_conv.conv.weight                             (48, 24, 2, 2, 2)\n",
      "decoder2.conv_block.conv1.conv.weight                        (24, 48, 3, 3, 3)\n",
      "decoder2.conv_block.conv2.conv.weight                        (24, 24, 3, 3, 3)\n",
      "decoder2.conv_block.conv3.conv.weight                        (24, 48, 1, 1, 1)\n",
      "decoder1.transp_conv.conv.weight                             (24, 24, 2, 2, 2)\n",
      "decoder1.conv_block.conv1.conv.weight                        (24, 48, 3, 3, 3)\n",
      "decoder1.conv_block.conv2.conv.weight                        (24, 24, 3, 3, 3)\n",
      "decoder1.conv_block.conv3.conv.weight                        (24, 48, 1, 1, 1)\n",
      "out.conv.conv.weight                                         (4, 24, 1, 1, 1)\n",
      "out.conv.conv.bias                                           (4,)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# ‚úÖ 1. Show what's in the YAML config\n",
    "pprint.pprint(inference_cfg)\n",
    "\n",
    "# ‚úÖ 2. Load local checkpoint\n",
    "checkpoint_path = \"../data/model_checkpoints/4-17-17/swinunetr-epoch=159.ckpt\"\n",
    "logger.info(f\"üìÅ Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# ‚úÖ 3. Print top-level keys in checkpoint\n",
    "print(\"üì¶ Checkpoint top-level keys:\", checkpoint.keys())\n",
    "\n",
    "# ‚úÖ 4. Extract state_dict if checkpoint is from PyTorch Lightning\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint  # regular PyTorch\n",
    "\n",
    "# ‚úÖ 5. Remove 'model.' prefix if present\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# ‚úÖ 6. Inspect parameter shapes in the state_dict\n",
    "print(\"\\nüîç Inspecting state_dict parameter shapes:\")\n",
    "for k, v in state_dict.items():\n",
    "    print(f\"{k:60s} {tuple(v.shape)}\")\n",
    "\n",
    "# ‚úÖ 7. Define model hyperparameters manually (since config file doesn't include them)\n",
    "hparams = {\n",
    "    'roi': {'h': 224, 'w': 224, 'd': 96},\n",
    "    'feature_size': 24,\n",
    "    'drop_rate': 0.01,\n",
    "    'attn_drop_rate': 0.01,\n",
    "    'dropout_path_rate': 0.01,\n",
    "    'depths': [2, 2, 2, 2],\n",
    "    'num_heads': [3, 6, 6, 6],\n",
    "    'norm_name': 'instance',\n",
    "    'normalize': True,\n",
    "    'downsample': 'merging',\n",
    "    'use_v2': False,\n",
    "    'mlp_ratio': 4,\n",
    "    'qkv_bias': True,\n",
    "    'patch_size': 2,\n",
    "    'window_size': 3\n",
    "}\n",
    "\n",
    "# ‚úÖ 8. Match output channels to checkpoint\n",
    "out_channels = 4  # based on your checkpoint, not 3\n",
    "\n",
    "# ‚úÖ 9. Convert ROI dict to list\n",
    "def listify_3d(x: dict):\n",
    "    return [x['h'], x['w'], x['d']]\n",
    "\n",
    "roi_size = listify_3d(hparams['roi'])\n",
    "logger.info(f\"üìê ROI size set to: {roi_size}\")\n",
    "\n",
    "# ‚úÖ 10. Instantiate model\n",
    "model_instance = model.CustomSwinUNETR(\n",
    "    in_channels=4,\n",
    "    img_size=roi_size,\n",
    "    out_channels=out_channels,\n",
    "    feature_size=hparams['feature_size'],\n",
    "    use_checkpoint=True,\n",
    "    depths=hparams['depths'],\n",
    "    num_heads=hparams['num_heads'],\n",
    "    norm_name=hparams['norm_name'],\n",
    "    normalize=hparams['normalize'],\n",
    "    downsample=hparams['downsample'],\n",
    "    use_v2=hparams['use_v2'],\n",
    "    mlp_ratio=hparams['mlp_ratio'],\n",
    "    qkv_bias=hparams['qkv_bias'],\n",
    "    patch_size=hparams['patch_size'],\n",
    "    window_size=hparams['window_size'],\n",
    ")\n",
    "\n",
    "# ‚úÖ 11. Load model weights with strict=False to allow shape mismatch fallback\n",
    "logger.info(\"üß† Loading model weights with strict=False (to ignore size mismatches)...\")\n",
    "model_instance.load_state_dict(state_dict, strict=False)\n",
    "model_instance.to(device)\n",
    "model_instance.eval()\n",
    "logger.info(\"‚úÖ Custom checkpoint loaded and model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9E4uJj9NJTn"
   },
   "source": [
    "## Load\n",
    "Once the model has been downloaded, this section loads the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujk2UMcINJTn",
    "outputId": "e29fca60-dc4b-4df8-b32d-d9f147c95a75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef listify_3d(x: dict):\\n    logger.info('üßä Listifying 3D dimensions...')\\n    dimensions = [x['h'], x['w'], x['d']]\\n    logger.debug(f'üìè Listified dimensions: {dimensions = }')\\n    return dimensions\\n\\nlogger.info('üéõÔ∏è Fetching hyperparameters...')\\nhparams = model_cfg['hyperparameter']\\nlogger.debug(f'üéõÔ∏è Hyperparameters: {hparams = }')\\n\\nlogger.info('üîó Fetching label union...')\\nunion = model_cfg['data']['label_union']\\nlogger.debug(f'üîó Label union: {union = }')\\n\\nlogger.info('üìê Calculating ROI size...')\\nroi_size = listify_3d(hparams['roi'])\\nlogger.debug(f'üìê ROI size: {roi_size = }')\\n\\nlogger.info('üèóÔ∏è Creating model instance...')\\nmodel = model.CustomSwinUNETR(\\n    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\\n    img_size          = roi_size,\\n    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\\n    feature_size      = hparams['feature_size'],\\n    use_checkpoint    = True,\\n    depths            = hparams['depths'],\\n    num_heads         = hparams['num_heads'],\\n    norm_name         = hparams['norm_name'],\\n    normalize         = hparams['normalize'],\\n    downsample        = hparams['downsample'],\\n    use_v2            = hparams['use_v2'],\\n    mlp_ratio         = hparams['mlp_ratio'],\\n    qkv_bias          = hparams['qkv_bias'],\\n    patch_size        = hparams['patch_size'],\\n    window_size       = hparams['window_size'],\\n)\\nlogger.info('‚úÖ Model instance created.')\\n\\nlogger.info('üîë Fetching first model state key...')\\nfirst_model_state_key = next(iter(model.state_dict().keys()))\\nlogger.debug(f'‚úÖ First model state key: {first_model_state_key = }')\\n\\nlogger.info('üîë Fetching first state dict key...')\\nfirst_state_dict_key = next(iter(state_dict.keys()))\\nlogger.debug(f'‚úÖ First state dict key: {first_state_dict_key = }')\\n\\nlogger.info('üíæ Loading state dictionary into model...')\\nmodel.load_state_dict(state_dict)\\nlogger.info('‚úÖ State dictionary loaded.')\\n\\nlogger.info('üñ•Ô∏è Moving model to device...')\\nmodel.to(device)\\nlogger.info('‚úÖ Model moved to device.')\\n\\nlogger.info('üß† Setting model to evaluation mode...')\\nmodel.eval()\\nlogger.info('‚úÖ Model set to evaluation mode.')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def listify_3d(x: dict):\n",
    "    logger.info('üßä Listifying 3D dimensions...')\n",
    "    dimensions = [x['h'], x['w'], x['d']]\n",
    "    logger.debug(f'üìè Listified dimensions: {dimensions = }')\n",
    "    return dimensions\n",
    "\n",
    "logger.info('üéõÔ∏è Fetching hyperparameters...')\n",
    "hparams = model_cfg['hyperparameter']\n",
    "logger.debug(f'üéõÔ∏è Hyperparameters: {hparams = }')\n",
    "\n",
    "logger.info('üîó Fetching label union...')\n",
    "union = model_cfg['data']['label_union']\n",
    "logger.debug(f'üîó Label union: {union = }')\n",
    "\n",
    "logger.info('üìê Calculating ROI size...')\n",
    "roi_size = listify_3d(hparams['roi'])\n",
    "logger.debug(f'üìê ROI size: {roi_size = }')\n",
    "\n",
    "logger.info('üèóÔ∏è Creating model instance...')\n",
    "model = model.CustomSwinUNETR(\n",
    "    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\n",
    "    img_size          = roi_size,\n",
    "    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\n",
    "    feature_size      = hparams['feature_size'],\n",
    "    use_checkpoint    = True,\n",
    "    depths            = hparams['depths'],\n",
    "    num_heads         = hparams['num_heads'],\n",
    "    norm_name         = hparams['norm_name'],\n",
    "    normalize         = hparams['normalize'],\n",
    "    downsample        = hparams['downsample'],\n",
    "    use_v2            = hparams['use_v2'],\n",
    "    mlp_ratio         = hparams['mlp_ratio'],\n",
    "    qkv_bias          = hparams['qkv_bias'],\n",
    "    patch_size        = hparams['patch_size'],\n",
    "    window_size       = hparams['window_size'],\n",
    ")\n",
    "logger.info('‚úÖ Model instance created.')\n",
    "\n",
    "logger.info('üîë Fetching first model state key...')\n",
    "first_model_state_key = next(iter(model.state_dict().keys()))\n",
    "logger.debug(f'‚úÖ First model state key: {first_model_state_key = }')\n",
    "\n",
    "logger.info('üîë Fetching first state dict key...')\n",
    "first_state_dict_key = next(iter(state_dict.keys()))\n",
    "logger.debug(f'‚úÖ First state dict key: {first_state_dict_key = }')\n",
    "\n",
    "logger.info('üíæ Loading state dictionary into model...')\n",
    "model.load_state_dict(state_dict)\n",
    "logger.info('‚úÖ State dictionary loaded.')\n",
    "\n",
    "logger.info('üñ•Ô∏è Moving model to device...')\n",
    "model.to(device)\n",
    "logger.info('‚úÖ Model moved to device.')\n",
    "\n",
    "logger.info('üß† Setting model to evaluation mode...')\n",
    "model.eval()\n",
    "logger.info('‚úÖ Model set to evaluation mode.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlZMp0kNJTn"
   },
   "source": [
    "# Data\n",
    "This section loads the data defined in your `config/config.yaml` file. Please see the example config file for more details. It requires you to have a nifti file for each MRI modality, typically T1, T2, T1-Contrast, and FLAIR, and the location to store the output nifti segmentation file. There are also instructions for loading multiple scans from a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFiR4fUuNJTo",
    "outputId": "d75fb267-4a8b-4974-91b8-6fc54694707d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,322 - brain_segmentation - INFO - üî¢ Setting up fold and configuration...\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - DEBUG - üìä fold = 1\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - INFO - üîç Checking optional configuration keys...\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - DEBUG - üîç Checking for key: compute_dice with value: True\n",
      "2025-09-23 03:11:01,324 - brain_segmentation - DEBUG - üîç Result of check: False\n",
      "2025-09-23 03:11:01,324 - brain_segmentation - DEBUG - üéØ do_ground_truth = False\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - üîç Checking for key: mode with value: multi\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - üîç Result of check: True\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - üìã use_scan_list = True\n",
      "2025-09-23 03:11:01,326 - brain_segmentation - DEBUG - üìÇ data_dir = '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed'\n",
      "2025-09-23 03:11:01,326 - brain_segmentation - INFO - üìÅ Loading scan data...\n",
      "2025-09-23 03:11:01,327 - brain_segmentation - INFO - üìö Using multiple scans from JSON...\n",
      "2025-09-23 03:11:01,332 - brain_segmentation - INFO - üìè Loading image size...\n",
      "2025-09-23 03:11:01,342 - brain_segmentation - DEBUG - üìê Scan size: resize_shape = [240, 240, 155]\n",
      "2025-09-23 03:11:01,343 - brain_segmentation - INFO - üîÑ Setting up validation data loader...\n",
      "2025-09-23 03:11:01,345 - brain_segmentation - INFO - ‚úÖ Validation data loader setup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 0\n",
      "Validation set size: 298\n"
     ]
    }
   ],
   "source": [
    "# üîß Utility to safely extract optional boolean keys from nested config\n",
    "def check_optional_key(x: dict, key_name, true_val):\n",
    "    logger.debug(f'üîç Checking for key: {key_name} with value: {true_val}')\n",
    "    result = (key_name in x.keys()) and (x[key_name] == true_val)\n",
    "    logger.debug(f'üîç Result of check: {result}')\n",
    "    return result\n",
    "\n",
    "\n",
    "logger.info('üî¢ Setting up fold and configuration...')\n",
    "fold = 1  # Let the validation fold be 1 - same convention as during training.\n",
    "logger.debug(f'üìä {fold = }')\n",
    "\n",
    "logger.info('üîç Checking optional configuration keys...')\n",
    "do_ground_truth: bool = check_optional_key(inference_cfg['input'], 'compute_dice', True)\n",
    "logger.debug(f'üéØ {do_ground_truth = }')\n",
    "use_scan_list: bool = check_optional_key(inference_cfg['input'], 'mode', 'multi')\n",
    "logger.debug(f'üìã {use_scan_list = }')\n",
    "\n",
    "data_dir = inference_cfg['input']['data_dir']\n",
    "logger.debug(f'üìÇ {data_dir = }')\n",
    "\n",
    "logger.info('üìÅ Loading scan data...')\n",
    "if use_scan_list:\n",
    "    logger.info('üìö Using multiple scans from JSON...')\n",
    "    json_path = inference_cfg['input']['multi_scan']['scan_list']\n",
    "    _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "    with open(json_path) as f:\n",
    "        test_instance = json.load(f)['training'][0]['image'][2]  # To get image size, grab the T1 scan.\n",
    "else:\n",
    "    logger.info('üñºÔ∏è Using single scan...')\n",
    "    test_instance = inference_cfg['input']['single_scan']['t1']  # To get image size, grab the T1 scan.\n",
    "    json_data = {\n",
    "        'training': [\n",
    "            {\n",
    "                'fold': fold,\n",
    "                'image': [\n",
    "                    inference_cfg['input']['single_scan']['flair'],\n",
    "                    inference_cfg['input']['single_scan']['t1c'],\n",
    "                    inference_cfg['input']['single_scan']['t1'],\n",
    "                    inference_cfg['input']['single_scan']['t2']\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if do_ground_truth:\n",
    "        json_data['training'][0]['label'] = inference_cfg['input']['single_scan']['ground_truth']\n",
    "\n",
    "    logger.info('üìù Creating temporary JSON file...')\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:  # Use delete=False to fix a permissions error on Windows.\n",
    "        json.dump(json_data, temp_file, indent=4)\n",
    "        temp_file.flush()\n",
    "        json_path = temp_file.name\n",
    "        _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "        temp_file.close()\n",
    "        os.unlink(temp_file.name)\n",
    "    logger.info('‚úÖ Temporary JSON file created and processed.')\n",
    "\n",
    "logger.info('üìè Loading image size...')\n",
    "resize_shape = list(nib.load(os.path.join(data_dir, test_instance)).shape)\n",
    "logger.debug(f'üìê Scan size: {resize_shape = }')\n",
    "\n",
    "logger.info('üîÑ Setting up validation data loader...')\n",
    "val_loader = get_loader_val(\n",
    "    batch_size=1,\n",
    "    files=validation_files,\n",
    "    val_resize=resize_shape,\n",
    "    # union=union,\n",
    "    workers=1,\n",
    "    cache_dir='',\n",
    "    dataset_type='Dataset',\n",
    "    add_label=do_ground_truth,\n",
    ")\n",
    "logger.info('‚úÖ Validation data loader setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_etOfVK4NJTo"
   },
   "source": [
    "# Inference\n",
    "This section takes the loaded model and runs inference on loaded scans, and saves the output to the location specified in your `config/config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dice score and output it\n",
    "import csv \n",
    "import os\n",
    "\n",
    "def save_dice_scores_to_csv(dice_scores, case_ids, output_csv_path):\n",
    "    \"\"\"\n",
    "    Save per-case Dice scores to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dice_scores (List[List[float]]): A list of [DICE_tc, DICE_wt, DICE_et, DICE_mean] for each case.\n",
    "        case_ids (List[str]): A list of case IDs corresponding to each Dice score row.\n",
    "        output_csv_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    with open(output_csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['CaseID', 'DICE_tc', 'DICE_wt', 'DICE_et', 'DICE_mean'])\n",
    "        for case_id, score in zip(case_ids, dice_scores):\n",
    "            writer.writerow([case_id] + score)\n",
    "\n",
    "    logger.info(f'üìÑ Dice scores saved to {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EH4JxkCUNJTo",
    "outputId": "c2c40d5c-e52d-44bc-e34d-01fe7d697f2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,358 - brain_segmentation - INFO - üìÇ Output directory set to: ../outputs/output_0923_time1_03\n",
      "2025-09-23 03:11:01,367 - brain_segmentation - INFO - üßÆ Setting up accuracy function...\n",
      "2025-09-23 03:11:02,663 - brain_segmentation - INFO - üì• Loading data for case 0...\n",
      "2025-09-23 03:11:02,665 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:11:02,673 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:12:00,617 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:12:00,720 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:12:00,782 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8877345 voxels\n",
      "2025-09-23 03:12:00,782 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 23514 voxels\n",
      "2025-09-23 03:12:00,783 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 27141 voxels\n",
      "2025-09-23 03:12:00,783 - brain_segmentation - INFO - üìä Label 0: 99.43% of total volume\n",
      "2025-09-23 03:12:00,784 - brain_segmentation - INFO - üìä Label 2: 0.26% of total volume\n",
      "2025-09-23 03:12:00,784 - brain_segmentation - INFO - üìä Label 4: 0.30% of total volume\n",
      "2025-09-23 03:12:00,849 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100001_seg.nii.gz\n",
      "2025-09-23 03:12:00,850 - brain_segmentation - INFO - üì• Loading data for case 1...\n",
      "2025-09-23 03:12:00,851 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:12:00,876 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:12:01,330 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:13:01,232 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:13:01,307 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8919210 voxels\n",
      "2025-09-23 03:13:01,307 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 8790 voxels\n",
      "2025-09-23 03:13:01,308 - brain_segmentation - INFO - üìä Label 0: 99.90% of total volume\n",
      "2025-09-23 03:13:01,308 - brain_segmentation - INFO - üìä Label 2: 0.10% of total volume\n",
      "2025-09-23 03:13:01,359 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100002_seg.nii.gz\n",
      "2025-09-23 03:13:01,361 - brain_segmentation - INFO - üì• Loading data for case 2...\n",
      "2025-09-23 03:13:01,361 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:13:01,375 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:13:01,830 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:14:01,965 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:14:02,023 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8842830 voxels\n",
      "2025-09-23 03:14:02,024 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 74946 voxels\n",
      "2025-09-23 03:14:02,024 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 10224 voxels\n",
      "2025-09-23 03:14:02,025 - brain_segmentation - INFO - üìä Label 0: 99.05% of total volume\n",
      "2025-09-23 03:14:02,025 - brain_segmentation - INFO - üìä Label 2: 0.84% of total volume\n",
      "2025-09-23 03:14:02,026 - brain_segmentation - INFO - üìä Label 4: 0.11% of total volume\n",
      "2025-09-23 03:14:02,081 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100003_seg.nii.gz\n",
      "2025-09-23 03:14:02,082 - brain_segmentation - INFO - üì• Loading data for case 3...\n",
      "2025-09-23 03:14:02,082 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:14:02,111 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:14:02,563 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:15:01,802 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:15:01,859 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8896378 voxels\n",
      "2025-09-23 03:15:01,860 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 29722 voxels\n",
      "2025-09-23 03:15:01,860 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 1900 voxels\n",
      "2025-09-23 03:15:01,861 - brain_segmentation - INFO - üìä Label 0: 99.65% of total volume\n",
      "2025-09-23 03:15:01,861 - brain_segmentation - INFO - üìä Label 2: 0.33% of total volume\n",
      "2025-09-23 03:15:01,862 - brain_segmentation - INFO - üìä Label 4: 0.02% of total volume\n",
      "2025-09-23 03:15:01,920 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100004_seg.nii.gz\n",
      "2025-09-23 03:15:01,921 - brain_segmentation - INFO - üì• Loading data for case 4...\n",
      "2025-09-23 03:15:01,922 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:15:01,944 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:15:02,401 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:16:01,679 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:16:01,735 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8906694 voxels\n",
      "2025-09-23 03:16:01,736 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 6732 voxels\n",
      "2025-09-23 03:16:01,737 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 14574 voxels\n",
      "2025-09-23 03:16:01,737 - brain_segmentation - INFO - üìä Label 0: 99.76% of total volume\n",
      "2025-09-23 03:16:01,738 - brain_segmentation - INFO - üìä Label 2: 0.08% of total volume\n",
      "2025-09-23 03:16:01,738 - brain_segmentation - INFO - üìä Label 4: 0.16% of total volume\n",
      "2025-09-23 03:16:01,789 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100005_seg.nii.gz\n",
      "2025-09-23 03:16:01,790 - brain_segmentation - INFO - üì• Loading data for case 5...\n",
      "2025-09-23 03:16:01,790 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:16:01,803 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:16:02,255 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:17:01,414 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:17:01,470 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8865193 voxels\n",
      "2025-09-23 03:17:01,470 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 55836 voxels\n",
      "2025-09-23 03:17:01,471 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 6971 voxels\n",
      "2025-09-23 03:17:01,471 - brain_segmentation - INFO - üìä Label 0: 99.30% of total volume\n",
      "2025-09-23 03:17:01,472 - brain_segmentation - INFO - üìä Label 2: 0.63% of total volume\n",
      "2025-09-23 03:17:01,472 - brain_segmentation - INFO - üìä Label 4: 0.08% of total volume\n",
      "2025-09-23 03:17:01,521 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100006_seg.nii.gz\n",
      "2025-09-23 03:17:01,522 - brain_segmentation - INFO - üì• Loading data for case 6...\n",
      "2025-09-23 03:17:01,523 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:17:01,545 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:17:01,998 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:18:01,284 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:18:01,345 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8925092 voxels\n",
      "2025-09-23 03:18:01,346 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 2727 voxels\n",
      "2025-09-23 03:18:01,346 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 181 voxels\n",
      "2025-09-23 03:18:01,347 - brain_segmentation - INFO - üìä Label 0: 99.97% of total volume\n",
      "2025-09-23 03:18:01,347 - brain_segmentation - INFO - üìä Label 2: 0.03% of total volume\n",
      "2025-09-23 03:18:01,348 - brain_segmentation - INFO - üìä Label 4: 0.00% of total volume\n",
      "2025-09-23 03:18:01,391 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100007_seg.nii.gz\n",
      "2025-09-23 03:18:01,392 - brain_segmentation - INFO - üì• Loading data for case 7...\n",
      "2025-09-23 03:18:01,393 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:18:01,441 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:18:01,898 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:19:01,110 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:19:01,174 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8869969 voxels\n",
      "2025-09-23 03:19:01,175 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 52143 voxels\n",
      "2025-09-23 03:19:01,175 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 5888 voxels\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - üìä Label 0: 99.35% of total volume\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - üìä Label 2: 0.58% of total volume\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - üìä Label 4: 0.07% of total volume\n",
      "2025-09-23 03:19:01,237 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100008_seg.nii.gz\n",
      "2025-09-23 03:19:01,238 - brain_segmentation - INFO - üì• Loading data for case 8...\n",
      "2025-09-23 03:19:01,239 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:19:01,263 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:19:01,720 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:20:00,885 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:20:00,943 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8845751 voxels\n",
      "2025-09-23 03:20:00,944 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 44646 voxels\n",
      "2025-09-23 03:20:00,944 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 37603 voxels\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - üìä Label 0: 99.08% of total volume\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - üìä Label 2: 0.50% of total volume\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - üìä Label 4: 0.42% of total volume\n",
      "2025-09-23 03:20:01,014 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100009_seg.nii.gz\n",
      "2025-09-23 03:20:01,015 - brain_segmentation - INFO - üì• Loading data for case 9...\n",
      "2025-09-23 03:20:01,015 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:20:01,038 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:20:01,497 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:21:00,674 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:21:00,732 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8837814 voxels\n",
      "2025-09-23 03:21:00,732 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 68009 voxels\n",
      "2025-09-23 03:21:00,733 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 22177 voxels\n",
      "2025-09-23 03:21:00,734 - brain_segmentation - INFO - üìä Label 0: 98.99% of total volume\n",
      "2025-09-23 03:21:00,734 - brain_segmentation - INFO - üìä Label 2: 0.76% of total volume\n",
      "2025-09-23 03:21:00,736 - brain_segmentation - INFO - üìä Label 4: 0.25% of total volume\n",
      "2025-09-23 03:21:00,790 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100010_seg.nii.gz\n",
      "2025-09-23 03:21:00,791 - brain_segmentation - INFO - üì• Loading data for case 10...\n",
      "2025-09-23 03:21:00,792 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:21:00,811 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:21:01,266 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:22:00,522 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:22:00,580 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8913503 voxels\n",
      "2025-09-23 03:22:00,580 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 14089 voxels\n",
      "2025-09-23 03:22:00,581 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 408 voxels\n",
      "2025-09-23 03:22:00,581 - brain_segmentation - INFO - üìä Label 0: 99.84% of total volume\n",
      "2025-09-23 03:22:00,582 - brain_segmentation - INFO - üìä Label 2: 0.16% of total volume\n",
      "2025-09-23 03:22:00,582 - brain_segmentation - INFO - üìä Label 4: 0.00% of total volume\n",
      "2025-09-23 03:22:00,636 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100011_seg.nii.gz\n",
      "2025-09-23 03:22:00,637 - brain_segmentation - INFO - üì• Loading data for case 11...\n",
      "2025-09-23 03:22:00,637 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:22:00,663 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:22:01,116 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:23:00,326 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:23:00,388 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8899224 voxels\n",
      "2025-09-23 03:23:00,388 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 14750 voxels\n",
      "2025-09-23 03:23:00,389 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 14026 voxels\n",
      "2025-09-23 03:23:00,389 - brain_segmentation - INFO - üìä Label 0: 99.68% of total volume\n",
      "2025-09-23 03:23:00,390 - brain_segmentation - INFO - üìä Label 2: 0.17% of total volume\n",
      "2025-09-23 03:23:00,390 - brain_segmentation - INFO - üìä Label 4: 0.16% of total volume\n",
      "2025-09-23 03:23:00,450 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100012_seg.nii.gz\n",
      "2025-09-23 03:23:00,451 - brain_segmentation - INFO - üì• Loading data for case 12...\n",
      "2025-09-23 03:23:00,452 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:23:00,493 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:23:00,947 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:24:00,271 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:24:00,331 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8906254 voxels\n",
      "2025-09-23 03:24:00,332 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 16150 voxels\n",
      "2025-09-23 03:24:00,332 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 5596 voxels\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - üìä Label 0: 99.76% of total volume\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - üìä Label 2: 0.18% of total volume\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - üìä Label 4: 0.06% of total volume\n",
      "2025-09-23 03:24:00,391 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100013_seg.nii.gz\n",
      "2025-09-23 03:24:00,392 - brain_segmentation - INFO - üì• Loading data for case 13...\n",
      "2025-09-23 03:24:00,393 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:24:00,415 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:24:00,870 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:00,110 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:25:00,171 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8876548 voxels\n",
      "2025-09-23 03:25:00,171 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 48766 voxels\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 2686 voxels\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - üìä Label 0: 99.42% of total volume\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - üìä Label 2: 0.55% of total volume\n",
      "2025-09-23 03:25:00,173 - brain_segmentation - INFO - üìä Label 4: 0.03% of total volume\n",
      "2025-09-23 03:25:00,215 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100014_seg.nii.gz\n",
      "2025-09-23 03:25:00,217 - brain_segmentation - INFO - üì• Loading data for case 14...\n",
      "2025-09-23 03:25:00,218 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:00,232 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:25:00,688 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:59,948 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:26:00,009 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8859275 voxels\n",
      "2025-09-23 03:26:00,010 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 66315 voxels\n",
      "2025-09-23 03:26:00,010 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 2410 voxels\n",
      "2025-09-23 03:26:00,011 - brain_segmentation - INFO - üìä Label 0: 99.23% of total volume\n",
      "2025-09-23 03:26:00,011 - brain_segmentation - INFO - üìä Label 2: 0.74% of total volume\n",
      "2025-09-23 03:26:00,012 - brain_segmentation - INFO - üìä Label 4: 0.03% of total volume\n",
      "2025-09-23 03:26:00,060 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100015_seg.nii.gz\n",
      "2025-09-23 03:26:00,060 - brain_segmentation - INFO - üì• Loading data for case 15...\n",
      "2025-09-23 03:26:00,061 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:00,082 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:26:00,541 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:59,719 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:26:59,778 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8919303 voxels\n",
      "2025-09-23 03:26:59,779 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 6544 voxels\n",
      "2025-09-23 03:26:59,779 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 2153 voxels\n",
      "2025-09-23 03:26:59,780 - brain_segmentation - INFO - üìä Label 0: 99.90% of total volume\n",
      "2025-09-23 03:26:59,780 - brain_segmentation - INFO - üìä Label 2: 0.07% of total volume\n",
      "2025-09-23 03:26:59,781 - brain_segmentation - INFO - üìä Label 4: 0.02% of total volume\n",
      "2025-09-23 03:26:59,838 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100016_seg.nii.gz\n",
      "2025-09-23 03:26:59,839 - brain_segmentation - INFO - üì• Loading data for case 16...\n",
      "2025-09-23 03:26:59,840 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:59,857 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:27:00,318 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:27:59,460 - brain_segmentation - INFO - üß† Generating segmentation mask...\n",
      "2025-09-23 03:27:59,520 - brain_segmentation - INFO - üè∑Ô∏è Label 0: 8916354 voxels\n",
      "2025-09-23 03:27:59,521 - brain_segmentation - INFO - üè∑Ô∏è Label 2: 10016 voxels\n",
      "2025-09-23 03:27:59,521 - brain_segmentation - INFO - üè∑Ô∏è Label 4: 1630 voxels\n",
      "2025-09-23 03:27:59,522 - brain_segmentation - INFO - üìä Label 0: 99.87% of total volume\n",
      "2025-09-23 03:27:59,522 - brain_segmentation - INFO - üìä Label 2: 0.11% of total volume\n",
      "2025-09-23 03:27:59,523 - brain_segmentation - INFO - üìä Label 4: 0.02% of total volume\n",
      "2025-09-23 03:27:59,562 - brain_segmentation - INFO - üíæ Saved segmentation to: ../outputs/output_0923_time1_03/100017_seg.nii.gz\n",
      "2025-09-23 03:27:59,563 - brain_segmentation - INFO - üì• Loading data for case 17...\n",
      "2025-09-23 03:27:59,563 - brain_segmentation - DEBUG - üñºÔ∏è val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:27:59,575 - brain_segmentation - DEBUG - üìê affine.shape = (4, 4)\n",
      "2025-09-23 03:28:00,028 - brain_segmentation - DEBUG - üî¢ val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../outputs/output_0923_time1_03\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"üìÇ Output directory set to: {output_dir}\")\n",
    "\n",
    "case_ids = []\n",
    "dice_scores = []\n",
    "\n",
    "# üîÅ If using scan list, load and filter it once outside the loop\n",
    "if use_scan_list:\n",
    "    with open(json_path) as f:\n",
    "        d = json.load(f)\n",
    "    scan_entries = [entry for entry in d['training'] if entry['fold'] == fold]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logger.info('üßÆ Setting up accuracy function...')\n",
    "    acc_func = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH)\n",
    "\n",
    "    loop_data = zip(val_loader, scan_entries) if use_scan_list else enumerate(val_loader)\n",
    "\n",
    "    for i, data in enumerate(loop_data):\n",
    "        val_data = data[0]\n",
    "        scan_entry = data[1] if use_scan_list else None\n",
    "\n",
    "        logger.info(f'üì• Loading data for case {i}...')\n",
    "        val_images = val_data['image']\n",
    "        logger.debug(f'üñºÔ∏è val_images.shape = {val_images.shape}')\n",
    "\n",
    "        # üîç Load affine matrix\n",
    "        if use_scan_list:\n",
    "            affine_path = os.path.join(data_dir, scan_entry['image'][2])\n",
    "            case_id = os.path.basename(os.path.dirname(affine_path))\n",
    "        else:\n",
    "            affine_path = os.path.join(data_dir, inference_cfg['input']['single_scan']['t1'])\n",
    "            case_id = 'single_scan'\n",
    "\n",
    "        affine = nib.load(affine_path).affine\n",
    "        logger.debug(f'üìê affine.shape = {affine.shape}')\n",
    "\n",
    "        # üîÑ Perform inference\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_images.to(device),\n",
    "            roi_size=roi_size,\n",
    "            sw_batch_size=4,\n",
    "            predictor=model_instance,\n",
    "        )\n",
    "        logger.debug(f'üî¢ val_outputs.shape = {val_outputs.shape}')\n",
    "\n",
    "        # üí° Post-processing predictions\n",
    "        val_outputs = val_outputs.cpu()[0]  # remove batch dim: [C, H, W, D]\n",
    "        val_outputs_per_channel = torch.unbind(val_outputs, dim=0)\n",
    "        post_sigmoid = Activations(sigmoid=True)\n",
    "        post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "        val_outputs_convert = [post_pred(post_sigmoid(c)) for c in val_outputs_per_channel]\n",
    "\n",
    "        # üßÆ DICE calculation\n",
    "        if do_ground_truth:\n",
    "            logger.info('üìä Calculating DICE scores...')\n",
    "            ground_truth = val_data['label']\n",
    "            acc_func.reset()\n",
    "            val_outputs_used = val_outputs_convert[:-1] if len(val_outputs_convert) > 3 else val_outputs_convert\n",
    "            val_outputs_used = [v.to(device) for v in val_outputs_used]\n",
    "            y_pred_tensor = torch.stack(val_outputs_used, dim=0).unsqueeze(0)  # shape [1, C, H, W, D]\n",
    "            acc_func(y_pred=y_pred_tensor, y=ground_truth.to(device))\n",
    "            acc = acc_func.aggregate().cpu().numpy()\n",
    "\n",
    "            num_zeroes = list(acc[:3]).count(0.0)\n",
    "            mean = (acc[0] + acc[1] + acc[2]) / (3 - num_zeroes) if num_zeroes < 3 else 0\n",
    "            logger.info(f'üìä DICE (tc): {acc[0]}')\n",
    "            logger.info(f'üìä DICE (wt): {acc[1]}')\n",
    "            logger.info(f'üìä DICE (et): {acc[2]}')\n",
    "            logger.info(f'üìä DICE (mean): {mean}')\n",
    "\n",
    "            dice_scores.append([acc[0], acc[1], acc[2], mean])\n",
    "            case_ids.append(case_id)\n",
    "\n",
    "        # üß† Segmentation mask generation (channel-wise, no argmax)\n",
    "        logger.info('üß† Generating segmentation mask...')\n",
    "        segmentation_mask = val_outputs_convert[:-1] if len(val_outputs_convert) > 3 else val_outputs_convert\n",
    "\n",
    "        # You can update this map based on what you want each channel to represent\n",
    "        channel_label_map = {0: 0, 1: 2, 2: 4, 3: 3, 4:1}  # e.g. 1 = NCR, 2 = SNFH, 3 = ET\n",
    "\n",
    "        remapped = np.zeros_like(segmentation_mask[0].cpu().numpy(), dtype=np.uint8)\n",
    "        for i, mask in enumerate(segmentation_mask):\n",
    "            binary_mask = mask.cpu().numpy().astype(bool)\n",
    "            if i in channel_label_map:\n",
    "                remapped[binary_mask] = channel_label_map[i]\n",
    "\n",
    "        segmentation_mask = remapped\n",
    "\n",
    "        # üßæ Log label distribution\n",
    "        unique_labels, label_counts = np.unique(segmentation_mask, return_counts=True)\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            logger.info(f'üè∑Ô∏è Label {int(label)}: {count} voxels')\n",
    "\n",
    "        total_voxels = np.prod(segmentation_mask.shape)\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            percentage = (count / total_voxels) * 100\n",
    "            logger.info(f'üìä Label {int(label)}: {percentage:.2f}% of total volume')\n",
    "\n",
    "        # üíæ Save NIfTI\n",
    "        save_pth = os.path.join(output_dir, f'{case_id}_seg.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(segmentation_mask.astype(np.uint8), affine=affine), save_pth)\n",
    "        logger.info(f'üíæ Saved segmentation to: {save_pth}')\n",
    "\n",
    "# ‚úÖ Final Dice summary\n",
    "if do_ground_truth and dice_scores:\n",
    "    logger.info('üìä Calculating mean DICE scores...')\n",
    "    mask = np.ma.masked_equal(dice_scores, 0)\n",
    "    mean = mask.mean(axis=0).filled(np.nan)\n",
    "    logger.info(f'üìä DICE (tc): {mean[0]}')\n",
    "    logger.info(f'üìä DICE (wt): {mean[1]}')\n",
    "    logger.info(f'üìä DICE (et): {mean[2]}')\n",
    "    logger.info(f'üìä DICE (mean): {mean[3]}')\n",
    "\n",
    "    # üíæ Save per-case DICE to CSV\n",
    "    logger.info(\"üíæ Saving per-case DICE scores to CSV...\")\n",
    "    dice_df = pd.DataFrame(dice_scores, columns=[\"DICE_tc\", \"DICE_wt\", \"DICE_et\", \"DICE_mean\"])\n",
    "    dice_df.insert(0, \"Case_ID\", case_ids)\n",
    "    dice_df.to_csv(os.path.join(output_dir, \"dice_scores.csv\"), index=False)\n",
    "    logger.info(f\"üìÑ Saved per-case DICE scores to: {os.path.join(output_dir, 'dice_scores.csv')}\")\n",
    "\n",
    "    # üíæ Save mean DICE score to CSV\n",
    "    mean_df = pd.DataFrame([{\n",
    "        \"DICE_tc\": mean[0],\n",
    "        \"DICE_wt\": mean[1],\n",
    "        \"DICE_et\": mean[2],\n",
    "        \"DICE_mean\": mean[3],\n",
    "    }])\n",
    "    mean_df.to_csv(os.path.join(output_dir, \"mean_dice_score.csv\"), index=False)\n",
    "    logger.info(f\"üìÑ Saved mean DICE score to: {os.path.join(output_dir, 'mean_dice_score.csv')}\")\n",
    "\n",
    "logger.info('‚úÖ Inference complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, mask in enumerate(segmentation_mask):\n",
    "#     print(f\"Element {i} type: {type(mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask_sum = torch.sum(torch.stack(segmentation_mask, dim=0), dim=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
