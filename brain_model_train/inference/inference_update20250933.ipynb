{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9O2FMEC9OeU"
   },
   "source": [
    "# Overview\n",
    "**BRAIN SEGMENTATION INFERENCE**\n",
    "This Jupyter notebook is designed to run inference on a brain MRI scan using a pre-trained segmentation model. It downloads a model from the cloud (requiring AWS access keys to be set up properly) and then runs inference on a single scan. The output is a nifti file with the segmentation labels. You may wish to then load it in a program such as Slicer to view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjoOMwOTNJTd"
   },
   "source": [
    "# Setup\n",
    "First, set up a python environment with the necessary packages.\n",
    "\n",
    "For instance, in VSCode or Cursor, press Ctrl+Shift+P and type \"Python: Create Environment\" and follow the prompts.\n",
    "\n",
    "Or, on the command line, run:\n",
    "```bash\n",
    "python -m venv brain_segmentation_inference_env\n",
    "```\n",
    "--or--\n",
    "```bash\n",
    "python3 -m venv brain_segmentation_inference_env\n",
    "```\n",
    "and then either \n",
    "```bash\n",
    "brain_segmentation_inference_env\\Scripts\\activate\n",
    "```\n",
    "on Windows, or\n",
    "```bash\n",
    "source brain_segmentation_inference_env/bin/activate\n",
    "```\n",
    "on macOS and Linux.\n",
    "\n",
    "Then, install the necessary packages:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "After all that, this cell should run without error and import all the necessary packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 2)) (2.5.5)\n",
      "Requirement already satisfied: wandb in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 3)) (0.22.0)\n",
      "Requirement already satisfied: omegaconf in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 4)) (2.3.0)\n",
      "Requirement already satisfied: hydra-core in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: einops in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 6)) (0.8.1)\n",
      "Requirement already satisfied: pillow in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 7)) (11.2.1)\n",
      "Requirement already satisfied: boto3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 8)) (1.40.36)\n",
      "Requirement already satisfied: monai[nibabel] in /home/rainsun/miniconda3/lib/python3.13/site-packages (from -r ../requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.24 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (2.2.6)\n",
      "Requirement already satisfied: torch<2.7.0,>=2.4.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: nibabel in /home/rainsun/miniconda3/lib/python3.13/site-packages (from monai[nibabel]->-r ../requirements.txt (line 1)) (5.3.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>5.4 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (2025.5.1)\n",
      "Requirement already satisfied: torchmetrics>0.7.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>4.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pytorch-lightning->-r ../requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from wandb->-r ../requirements.txt (line 3)) (2.38.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/rainsun/miniconda3/lib/python3.13/site-packages (from omegaconf->-r ../requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.36 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (1.40.36)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from boto3->-r ../requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (2.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (3.12.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 3)) (4.0.12)\n",
      "Requirement already satisfied: setuptools in /home/rainsun/miniconda3/lib/python3.13/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning->-r ../requirements.txt (line 2)) (78.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from pydantic<3->wandb->-r ../requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb->-r ../requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: filelock in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: networkx in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->-r ../requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r ../requirements.txt (line 3)) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.36->boto3->-r ../requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/rainsun/miniconda3/lib/python3.13/site-packages (from jinja2->torch<2.7.0,>=2.4.1->monai[nibabel]->-r ../requirements.txt (line 1)) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "cuda devices (count) .... 1\n"
     ]
    }
   ],
   "source": [
    "#Check GPU availability\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print('cuda devices (count) ....',torch.cuda.device_count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 23 03:10:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.03              Driver Version: 576.28         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8              2W /  115W |     181MiB /   8188MiB |     17%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:10:49,886 - brain_segmentation - INFO - 🚀 Setting up logging...\n",
      "2025-09-23 03:10:49,887 - brain_segmentation - DEBUG - 🔧 Current logging level: 10\n",
      "2025-09-23 03:10:49,888 - brain_segmentation - INFO - ✅ Logging setup complete.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Create a named logger\n",
    "logger = logging.getLogger('brain_segmentation')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a console handler and set its level\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create a formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Add the formatter to the console handler\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the console handler to the logger\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# Turn off logs from all other loggers\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name != 'brain_segmentation':\n",
    "        logging.getLogger(name).setLevel(logging.CRITICAL)\n",
    "\n",
    "logger.info('🚀 Setting up logging...')\n",
    "logger.debug(f'🔧 Current logging level: {logger.getEffectiveLevel()}')\n",
    "logger.info('✅ Logging setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SdAAZeqmPNsv",
    "outputId": "2706f96f-1b8d-4f0d-9f3e-b0f3c8dbaa2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:10:49,894 - brain_segmentation - INFO - 📦 Importing outside packages...\n",
      "/home/rainsun/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-23 03:10:51,793 - brain_segmentation - INFO - ✅ Outside packages imported.\n",
      "2025-09-23 03:10:51,793 - brain_segmentation - INFO - 📦 Importing local packages...\n",
      "2025-09-23 03:10:51,802 - brain_segmentation - INFO - ✅ Local packages imported.\n",
      "2025-09-23 03:10:51,803 - brain_segmentation - INFO - 📦 Setting up device...\n",
      "2025-09-23 03:10:51,891 - brain_segmentation - INFO - ✅ Device set up.\n",
      "2025-09-23 03:10:51,891 - brain_segmentation - DEBUG - 🖥️ device = device(type='cuda', index=0)\n",
      "2025-09-23 03:10:51,894 - brain_segmentation - INFO - 📂 Output directory set to: data/output_my\n"
     ]
    }
   ],
   "source": [
    "logger.info('📦 Importing outside packages...')\n",
    "import os, sys\n",
    "import torch\n",
    "import glob\n",
    "import json, yaml\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import botocore\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from smart_open import open\n",
    "from monai.transforms import AsDiscrete, Activations\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.inferers import sliding_window_inference\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "logger.info('✅ Outside packages imported.')\n",
    "\n",
    "logger.info('📦 Importing local packages...')\n",
    "from core_common import get_loader_val, datafold_read\n",
    "import model\n",
    "logger.info('✅ Local packages imported.')\n",
    "\n",
    "logger.info('📦 Setting up device...')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info('✅ Device set up.')\n",
    "logger.debug(f'🖥️ {device = }')\n",
    "\n",
    "# make output directory\n",
    "output_dir = \"data/output_my\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"📂 Output directory set to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OeN_BW_NJTj"
   },
   "source": [
    "# Model\n",
    "This part loads the model from the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX5dkAr0NJTl"
   },
   "source": [
    "## Download\n",
    "This part downloads the model from the cloud.\n",
    "\n",
    "You'll need to have an AWS profile named 'theta-model-downloader'. You can create this profile by running this command in your terminal:\n",
    "```bash\n",
    "aws configure --profile theta-model-downloader\n",
    "```\n",
    "and entering your AWS credentials.\n",
    "\n",
    "If you do not have the AWS command line tools installed, you can install them by following the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-cli/2.30.1 Python/3.13.7 Linux/5.15.167.4-microsoft-standard-WSL2 exe/x86_64.ubuntu.24\n"
     ]
    }
   ],
   "source": [
    "!aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting the input data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter if you want to run inference on 'time1' or 'time2' time1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User choose to run inference on time1\n",
      "❌ Missing files for UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_flair.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t1ce.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t1.nii.gz\n",
      "   - /mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx/UCSF_PostopGlioma_Table S1 R1 V5.0_UNBLINDED_FINAL.xlsx_time1_t2.nii.gz\n",
      "\n",
      "✅ Total valid cases: 298\n",
      "🧪 Sample valid case IDs: ['100001', '100002', '100003', '100004', '100005']\n"
     ]
    }
   ],
   "source": [
    "# choose to run inference on time1 or time2\n",
    "time_num = input(\"Please enter if you want to run inference on 'time1' or 'time2'\")\n",
    "print(\"User choose to run inference on \" + time_num)\n",
    "'''main_path=\"/app/train/tumor_seg/tumor_seg/data/Preprocessed\"\n",
    "\n",
    "cases_list=glob.glob(os.path.join(main_path, \"*\"))\n",
    "cases_list=sorted(cases_list)\n",
    "\n",
    "new_ids=[]\n",
    "for paths_files in cases_list:\n",
    "    case_id=Path(paths_files)\n",
    "    case_id=case_id.parts[-3]\n",
    "    print(case_id)\n",
    "    dir_name=os.path.dirname(paths_files)\n",
    "    print(dir_name)\n",
    "    files = [dir_name+'/'+case_id+'_time1_flair.nii.gz', \n",
    "             dir_name+'/'+case_id + '_time1_t1ce.nii.gz',\n",
    "             dir_name+'/'+case_id + '_time1_t1.nii.gz', \n",
    "             dir_name+'/'+case_id + '_time1_t2.nii.gz']\n",
    "    \n",
    "    all_exist = all([os.path.exists(file) for file in files])\n",
    "    if all_exist:\n",
    "        new_ids.append(case_id)\n",
    "\n",
    "len(new_ids)'''\n",
    "main_path = \"/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed\"\n",
    "cases_list = sorted(glob.glob(os.path.join(main_path, \"*\")))\n",
    "\n",
    "new_ids = []\n",
    "\n",
    "for case_path in cases_list:\n",
    "    case_id = Path(case_path).name  # ✅ correct way to extract the folder name (e.g., \"100001\")\n",
    "    \n",
    "    '''\n",
    "    files = [\n",
    "        os.path.join(case_path, f\"{case_id}_time1_flair.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t1ce.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t1.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_time1_t2.nii.gz\"),\n",
    "    ]\n",
    "    '''\n",
    "    files = [\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_flair.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t1ce.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t1.nii.gz\"),\n",
    "        os.path.join(case_path, f\"{case_id}_{time_num}_t2.nii.gz\"),\n",
    "    ]\n",
    "\n",
    "    all_exist = all(os.path.exists(f) for f in files)\n",
    "    if all_exist:\n",
    "        new_ids.append(case_id)\n",
    "    else:\n",
    "        print(f\"❌ Missing files for {case_id}\")\n",
    "        for f in files:\n",
    "            if not os.path.exists(f):\n",
    "                print(f\"   - {f}\")\n",
    "\n",
    "print(f\"\\n✅ Total valid cases: {len(new_ids)}\")\n",
    "print(\"🧪 Sample valid case IDs:\", new_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnew_ids=[\"0171\", \"0178\", \"0179\" ,\"0190\", \"0196\", \"0198\", \"0199\",\"0245\",\\n         \"0281\",\"0296\", \"0325\", \"0338\", \"355\"]\\nnew_ids=[\"0190\"]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "new_ids=[\"0171\", \"0178\", \"0179\" ,\"0190\", \"0196\", \"0198\", \"0199\",\"0245\",\n",
    "         \"0281\",\"0296\", \"0325\", \"0338\", \"355\"]\n",
    "new_ids=[\"0190\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'training': []\n",
    "}\n",
    "\n",
    "for id_num in new_ids:\n",
    "    new_entry = {\n",
    "        'fold': 1,\n",
    "        'label': f'{id_num}/{id_num}_{time_num}_seg.nii.gz',\n",
    "        'image': [\n",
    "            f'{id_num}/{id_num}_{time_num}_flair.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t1ce.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t1.nii.gz',\n",
    "            f'{id_num}/{id_num}_{time_num}_t2.nii.gz'\n",
    "        ]\n",
    "    }\n",
    "    # Append the new entry to the 'training' list\n",
    "    data['training'].append(new_entry)\n",
    "    \n",
    "with open('./data/multi_example2.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fold': 1,\n",
       "  'label': '100001/100001_time1_seg.nii.gz',\n",
       "  'image': ['100001/100001_time1_flair.nii.gz',\n",
       "   '100001/100001_time1_t1ce.nii.gz',\n",
       "   '100001/100001_time1_t1.nii.gz',\n",
       "   '100001/100001_time1_t2.nii.gz']}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path=\"./data/multi_example2.json\"\n",
    "with open(files_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# check first instance JSON data\n",
    "data[\"training\"][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:00,336 - brain_segmentation - INFO - 📂 Loading configuration file...\n",
      "2025-09-23 03:11:00,336 - brain_segmentation - INFO - 🔓 Opening configuration file...\n",
      "2025-09-23 03:11:00,340 - brain_segmentation - INFO - 📖 Reading YAML content...\n",
      "2025-09-23 03:11:00,342 - brain_segmentation - DEBUG - 🔧 inference_cfg = {'input': {'compute_dice': False, 'mode': 'multi', 'multi_scan': {'scan_list': 'data/multi_example2.json'}, 'single_scan': {'ground_truth': '100001/100001_time1_seg.nii.gz', 'flair': '100001/100001_time1_flair.nii.gz', 't1c': '100001/100001_time1_t1ce.nii.gz', 't1': '100001/100001_time1_t1.nii.gz', 't2': '100001/100001_time1_t2.nii.gz'}, 'data_dir': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed'}, 'output': {'file_path': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/out_seg.nii.gz'}, 'model': {'bucket': 'theta-trained-models', 'key': 'model.ckpt', 'region': 'us-east-1'}}\n",
      "2025-09-23 03:11:00,343 - brain_segmentation - INFO - ✅ Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info('📂 Loading configuration file...')\n",
    "config_pth = '../config/example_config.yaml'\n",
    "\n",
    "logger.info('🔓 Opening configuration file...')\n",
    "with open(config_pth, 'r') as file:\n",
    "    logger.info('📖 Reading YAML content...')\n",
    "    inference_cfg = yaml.safe_load(file)\n",
    "logger.debug(f'🔧 {inference_cfg = }')\n",
    "logger.info('✅ Configuration loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the terminal\n",
    "#!aws configure --profile theta-model-downloader\n",
    "# enter your credential: (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb1qCskWNJTm",
    "outputId": "a8a36564-66cf-4cf1-c8e9-b7201c6f6b93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:00,351 - brain_segmentation - INFO - 📁 Loading checkpoint from ../data/model_checkpoints/4-17-17/swinunetr-epoch=159.ckpt...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'compute_dice': False,\n",
      "           'data_dir': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed',\n",
      "           'mode': 'multi',\n",
      "           'multi_scan': {'scan_list': 'data/multi_example2.json'},\n",
      "           'single_scan': {'flair': '100001/100001_time1_flair.nii.gz',\n",
      "                           'ground_truth': '100001/100001_time1_seg.nii.gz',\n",
      "                           't1': '100001/100001_time1_t1.nii.gz',\n",
      "                           't1c': '100001/100001_time1_t1ce.nii.gz',\n",
      "                           't2': '100001/100001_time1_t2.nii.gz'}},\n",
      " 'model': {'bucket': 'theta-trained-models',\n",
      "           'key': 'model.ckpt',\n",
      "           'region': 'us-east-1'},\n",
      " 'output': {'file_path': '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/out_seg.nii.gz'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,221 - brain_segmentation - INFO - 📐 ROI size set to: [224, 224, 96]\n",
      "2025-09-23 03:11:01,280 - brain_segmentation - INFO - 🧠 Loading model weights with strict=False (to ignore size mismatches)...\n",
      "2025-09-23 03:11:01,311 - brain_segmentation - INFO - ✅ Custom checkpoint loaded and model ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Checkpoint top-level keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "\n",
      "🔍 Inspecting state_dict parameter shapes:\n",
      "swinViT.patch_embed.proj.weight                              (24, 4, 2, 2, 2)\n",
      "swinViT.patch_embed.proj.bias                                (24,)\n",
      "swinViT.layers1.0.blocks.0.norm1.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.0.norm1.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.0.attn.relative_position_bias_table (125, 3)\n",
      "swinViT.layers1.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers1.0.blocks.0.attn.qkv.weight                   (72, 24)\n",
      "swinViT.layers1.0.blocks.0.attn.qkv.bias                     (72,)\n",
      "swinViT.layers1.0.blocks.0.attn.proj.weight                  (24, 24)\n",
      "swinViT.layers1.0.blocks.0.attn.proj.bias                    (24,)\n",
      "swinViT.layers1.0.blocks.0.norm2.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.0.norm2.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear1.weight                (96, 24)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear1.bias                  (96,)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear2.weight                (24, 96)\n",
      "swinViT.layers1.0.blocks.0.mlp.linear2.bias                  (24,)\n",
      "swinViT.layers1.0.blocks.1.norm1.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.1.norm1.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.1.attn.relative_position_bias_table (125, 3)\n",
      "swinViT.layers1.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers1.0.blocks.1.attn.qkv.weight                   (72, 24)\n",
      "swinViT.layers1.0.blocks.1.attn.qkv.bias                     (72,)\n",
      "swinViT.layers1.0.blocks.1.attn.proj.weight                  (24, 24)\n",
      "swinViT.layers1.0.blocks.1.attn.proj.bias                    (24,)\n",
      "swinViT.layers1.0.blocks.1.norm2.weight                      (24,)\n",
      "swinViT.layers1.0.blocks.1.norm2.bias                        (24,)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear1.weight                (96, 24)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear1.bias                  (96,)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear2.weight                (24, 96)\n",
      "swinViT.layers1.0.blocks.1.mlp.linear2.bias                  (24,)\n",
      "swinViT.layers1.0.downsample.reduction.weight                (48, 192)\n",
      "swinViT.layers1.0.downsample.norm.weight                     (192,)\n",
      "swinViT.layers1.0.downsample.norm.bias                       (192,)\n",
      "swinViT.layers2.0.blocks.0.norm1.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.0.norm1.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers2.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers2.0.blocks.0.attn.qkv.weight                   (144, 48)\n",
      "swinViT.layers2.0.blocks.0.attn.qkv.bias                     (144,)\n",
      "swinViT.layers2.0.blocks.0.attn.proj.weight                  (48, 48)\n",
      "swinViT.layers2.0.blocks.0.attn.proj.bias                    (48,)\n",
      "swinViT.layers2.0.blocks.0.norm2.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.0.norm2.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear1.weight                (192, 48)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear1.bias                  (192,)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear2.weight                (48, 192)\n",
      "swinViT.layers2.0.blocks.0.mlp.linear2.bias                  (48,)\n",
      "swinViT.layers2.0.blocks.1.norm1.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.1.norm1.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers2.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers2.0.blocks.1.attn.qkv.weight                   (144, 48)\n",
      "swinViT.layers2.0.blocks.1.attn.qkv.bias                     (144,)\n",
      "swinViT.layers2.0.blocks.1.attn.proj.weight                  (48, 48)\n",
      "swinViT.layers2.0.blocks.1.attn.proj.bias                    (48,)\n",
      "swinViT.layers2.0.blocks.1.norm2.weight                      (48,)\n",
      "swinViT.layers2.0.blocks.1.norm2.bias                        (48,)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear1.weight                (192, 48)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear1.bias                  (192,)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear2.weight                (48, 192)\n",
      "swinViT.layers2.0.blocks.1.mlp.linear2.bias                  (48,)\n",
      "swinViT.layers2.0.downsample.reduction.weight                (96, 384)\n",
      "swinViT.layers2.0.downsample.norm.weight                     (384,)\n",
      "swinViT.layers2.0.downsample.norm.bias                       (384,)\n",
      "swinViT.layers3.0.blocks.0.norm1.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.0.norm1.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers3.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers3.0.blocks.0.attn.qkv.weight                   (288, 96)\n",
      "swinViT.layers3.0.blocks.0.attn.qkv.bias                     (288,)\n",
      "swinViT.layers3.0.blocks.0.attn.proj.weight                  (96, 96)\n",
      "swinViT.layers3.0.blocks.0.attn.proj.bias                    (96,)\n",
      "swinViT.layers3.0.blocks.0.norm2.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.0.norm2.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear1.weight                (384, 96)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear1.bias                  (384,)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear2.weight                (96, 384)\n",
      "swinViT.layers3.0.blocks.0.mlp.linear2.bias                  (96,)\n",
      "swinViT.layers3.0.blocks.1.norm1.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.1.norm1.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers3.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers3.0.blocks.1.attn.qkv.weight                   (288, 96)\n",
      "swinViT.layers3.0.blocks.1.attn.qkv.bias                     (288,)\n",
      "swinViT.layers3.0.blocks.1.attn.proj.weight                  (96, 96)\n",
      "swinViT.layers3.0.blocks.1.attn.proj.bias                    (96,)\n",
      "swinViT.layers3.0.blocks.1.norm2.weight                      (96,)\n",
      "swinViT.layers3.0.blocks.1.norm2.bias                        (96,)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear1.weight                (384, 96)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear1.bias                  (384,)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear2.weight                (96, 384)\n",
      "swinViT.layers3.0.blocks.1.mlp.linear2.bias                  (96,)\n",
      "swinViT.layers3.0.downsample.reduction.weight                (192, 768)\n",
      "swinViT.layers3.0.downsample.norm.weight                     (768,)\n",
      "swinViT.layers3.0.downsample.norm.bias                       (768,)\n",
      "swinViT.layers4.0.blocks.0.norm1.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.0.norm1.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.0.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers4.0.blocks.0.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers4.0.blocks.0.attn.qkv.weight                   (576, 192)\n",
      "swinViT.layers4.0.blocks.0.attn.qkv.bias                     (576,)\n",
      "swinViT.layers4.0.blocks.0.attn.proj.weight                  (192, 192)\n",
      "swinViT.layers4.0.blocks.0.attn.proj.bias                    (192,)\n",
      "swinViT.layers4.0.blocks.0.norm2.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.0.norm2.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear1.weight                (768, 192)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear1.bias                  (768,)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear2.weight                (192, 768)\n",
      "swinViT.layers4.0.blocks.0.mlp.linear2.bias                  (192,)\n",
      "swinViT.layers4.0.blocks.1.norm1.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.1.norm1.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.1.attn.relative_position_bias_table (125, 6)\n",
      "swinViT.layers4.0.blocks.1.attn.relative_position_index      (27, 27)\n",
      "swinViT.layers4.0.blocks.1.attn.qkv.weight                   (576, 192)\n",
      "swinViT.layers4.0.blocks.1.attn.qkv.bias                     (576,)\n",
      "swinViT.layers4.0.blocks.1.attn.proj.weight                  (192, 192)\n",
      "swinViT.layers4.0.blocks.1.attn.proj.bias                    (192,)\n",
      "swinViT.layers4.0.blocks.1.norm2.weight                      (192,)\n",
      "swinViT.layers4.0.blocks.1.norm2.bias                        (192,)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear1.weight                (768, 192)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear1.bias                  (768,)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear2.weight                (192, 768)\n",
      "swinViT.layers4.0.blocks.1.mlp.linear2.bias                  (192,)\n",
      "swinViT.layers4.0.downsample.reduction.weight                (384, 1536)\n",
      "swinViT.layers4.0.downsample.norm.weight                     (1536,)\n",
      "swinViT.layers4.0.downsample.norm.bias                       (1536,)\n",
      "encoder1.layer.conv1.conv.weight                             (24, 4, 3, 3, 3)\n",
      "encoder1.layer.conv2.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder1.layer.conv3.conv.weight                             (24, 4, 1, 1, 1)\n",
      "encoder2.layer.conv1.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder2.layer.conv2.conv.weight                             (24, 24, 3, 3, 3)\n",
      "encoder3.layer.conv1.conv.weight                             (48, 48, 3, 3, 3)\n",
      "encoder3.layer.conv2.conv.weight                             (48, 48, 3, 3, 3)\n",
      "encoder4.layer.conv1.conv.weight                             (96, 96, 3, 3, 3)\n",
      "encoder4.layer.conv2.conv.weight                             (96, 96, 3, 3, 3)\n",
      "encoder10.layer.conv1.conv.weight                            (384, 384, 3, 3, 3)\n",
      "encoder10.layer.conv2.conv.weight                            (384, 384, 3, 3, 3)\n",
      "decoder5.transp_conv.conv.weight                             (384, 192, 2, 2, 2)\n",
      "decoder5.conv_block.conv1.conv.weight                        (192, 384, 3, 3, 3)\n",
      "decoder5.conv_block.conv2.conv.weight                        (192, 192, 3, 3, 3)\n",
      "decoder5.conv_block.conv3.conv.weight                        (192, 384, 1, 1, 1)\n",
      "decoder4.transp_conv.conv.weight                             (192, 96, 2, 2, 2)\n",
      "decoder4.conv_block.conv1.conv.weight                        (96, 192, 3, 3, 3)\n",
      "decoder4.conv_block.conv2.conv.weight                        (96, 96, 3, 3, 3)\n",
      "decoder4.conv_block.conv3.conv.weight                        (96, 192, 1, 1, 1)\n",
      "decoder3.transp_conv.conv.weight                             (96, 48, 2, 2, 2)\n",
      "decoder3.conv_block.conv1.conv.weight                        (48, 96, 3, 3, 3)\n",
      "decoder3.conv_block.conv2.conv.weight                        (48, 48, 3, 3, 3)\n",
      "decoder3.conv_block.conv3.conv.weight                        (48, 96, 1, 1, 1)\n",
      "decoder2.transp_conv.conv.weight                             (48, 24, 2, 2, 2)\n",
      "decoder2.conv_block.conv1.conv.weight                        (24, 48, 3, 3, 3)\n",
      "decoder2.conv_block.conv2.conv.weight                        (24, 24, 3, 3, 3)\n",
      "decoder2.conv_block.conv3.conv.weight                        (24, 48, 1, 1, 1)\n",
      "decoder1.transp_conv.conv.weight                             (24, 24, 2, 2, 2)\n",
      "decoder1.conv_block.conv1.conv.weight                        (24, 48, 3, 3, 3)\n",
      "decoder1.conv_block.conv2.conv.weight                        (24, 24, 3, 3, 3)\n",
      "decoder1.conv_block.conv3.conv.weight                        (24, 48, 1, 1, 1)\n",
      "out.conv.conv.weight                                         (4, 24, 1, 1, 1)\n",
      "out.conv.conv.bias                                           (4,)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# ✅ 1. Show what's in the YAML config\n",
    "pprint.pprint(inference_cfg)\n",
    "\n",
    "# ✅ 2. Load local checkpoint\n",
    "checkpoint_path = \"../data/model_checkpoints/4-17-17/swinunetr-epoch=159.ckpt\"\n",
    "logger.info(f\"📁 Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# ✅ 3. Print top-level keys in checkpoint\n",
    "print(\"📦 Checkpoint top-level keys:\", checkpoint.keys())\n",
    "\n",
    "# ✅ 4. Extract state_dict if checkpoint is from PyTorch Lightning\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:\n",
    "    state_dict = checkpoint  # regular PyTorch\n",
    "\n",
    "# ✅ 5. Remove 'model.' prefix if present\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "# ✅ 6. Inspect parameter shapes in the state_dict\n",
    "print(\"\\n🔍 Inspecting state_dict parameter shapes:\")\n",
    "for k, v in state_dict.items():\n",
    "    print(f\"{k:60s} {tuple(v.shape)}\")\n",
    "\n",
    "# ✅ 7. Define model hyperparameters manually (since config file doesn't include them)\n",
    "hparams = {\n",
    "    'roi': {'h': 224, 'w': 224, 'd': 96},\n",
    "    'feature_size': 24,\n",
    "    'drop_rate': 0.01,\n",
    "    'attn_drop_rate': 0.01,\n",
    "    'dropout_path_rate': 0.01,\n",
    "    'depths': [2, 2, 2, 2],\n",
    "    'num_heads': [3, 6, 6, 6],\n",
    "    'norm_name': 'instance',\n",
    "    'normalize': True,\n",
    "    'downsample': 'merging',\n",
    "    'use_v2': False,\n",
    "    'mlp_ratio': 4,\n",
    "    'qkv_bias': True,\n",
    "    'patch_size': 2,\n",
    "    'window_size': 3\n",
    "}\n",
    "\n",
    "# ✅ 8. Match output channels to checkpoint\n",
    "out_channels = 4  # based on your checkpoint, not 3\n",
    "\n",
    "# ✅ 9. Convert ROI dict to list\n",
    "def listify_3d(x: dict):\n",
    "    return [x['h'], x['w'], x['d']]\n",
    "\n",
    "roi_size = listify_3d(hparams['roi'])\n",
    "logger.info(f\"📐 ROI size set to: {roi_size}\")\n",
    "\n",
    "# ✅ 10. Instantiate model\n",
    "model_instance = model.CustomSwinUNETR(\n",
    "    in_channels=4,\n",
    "    img_size=roi_size,\n",
    "    out_channels=out_channels,\n",
    "    feature_size=hparams['feature_size'],\n",
    "    use_checkpoint=True,\n",
    "    depths=hparams['depths'],\n",
    "    num_heads=hparams['num_heads'],\n",
    "    norm_name=hparams['norm_name'],\n",
    "    normalize=hparams['normalize'],\n",
    "    downsample=hparams['downsample'],\n",
    "    use_v2=hparams['use_v2'],\n",
    "    mlp_ratio=hparams['mlp_ratio'],\n",
    "    qkv_bias=hparams['qkv_bias'],\n",
    "    patch_size=hparams['patch_size'],\n",
    "    window_size=hparams['window_size'],\n",
    ")\n",
    "\n",
    "# ✅ 11. Load model weights with strict=False to allow shape mismatch fallback\n",
    "logger.info(\"🧠 Loading model weights with strict=False (to ignore size mismatches)...\")\n",
    "model_instance.load_state_dict(state_dict, strict=False)\n",
    "model_instance.to(device)\n",
    "model_instance.eval()\n",
    "logger.info(\"✅ Custom checkpoint loaded and model ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9E4uJj9NJTn"
   },
   "source": [
    "## Load\n",
    "Once the model has been downloaded, this section loads the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ujk2UMcINJTn",
    "outputId": "e29fca60-dc4b-4df8-b32d-d9f147c95a75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef listify_3d(x: dict):\\n    logger.info('🧊 Listifying 3D dimensions...')\\n    dimensions = [x['h'], x['w'], x['d']]\\n    logger.debug(f'📏 Listified dimensions: {dimensions = }')\\n    return dimensions\\n\\nlogger.info('🎛️ Fetching hyperparameters...')\\nhparams = model_cfg['hyperparameter']\\nlogger.debug(f'🎛️ Hyperparameters: {hparams = }')\\n\\nlogger.info('🔗 Fetching label union...')\\nunion = model_cfg['data']['label_union']\\nlogger.debug(f'🔗 Label union: {union = }')\\n\\nlogger.info('📐 Calculating ROI size...')\\nroi_size = listify_3d(hparams['roi'])\\nlogger.debug(f'📐 ROI size: {roi_size = }')\\n\\nlogger.info('🏗️ Creating model instance...')\\nmodel = model.CustomSwinUNETR(\\n    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\\n    img_size          = roi_size,\\n    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\\n    feature_size      = hparams['feature_size'],\\n    use_checkpoint    = True,\\n    depths            = hparams['depths'],\\n    num_heads         = hparams['num_heads'],\\n    norm_name         = hparams['norm_name'],\\n    normalize         = hparams['normalize'],\\n    downsample        = hparams['downsample'],\\n    use_v2            = hparams['use_v2'],\\n    mlp_ratio         = hparams['mlp_ratio'],\\n    qkv_bias          = hparams['qkv_bias'],\\n    patch_size        = hparams['patch_size'],\\n    window_size       = hparams['window_size'],\\n)\\nlogger.info('✅ Model instance created.')\\n\\nlogger.info('🔑 Fetching first model state key...')\\nfirst_model_state_key = next(iter(model.state_dict().keys()))\\nlogger.debug(f'✅ First model state key: {first_model_state_key = }')\\n\\nlogger.info('🔑 Fetching first state dict key...')\\nfirst_state_dict_key = next(iter(state_dict.keys()))\\nlogger.debug(f'✅ First state dict key: {first_state_dict_key = }')\\n\\nlogger.info('💾 Loading state dictionary into model...')\\nmodel.load_state_dict(state_dict)\\nlogger.info('✅ State dictionary loaded.')\\n\\nlogger.info('🖥️ Moving model to device...')\\nmodel.to(device)\\nlogger.info('✅ Model moved to device.')\\n\\nlogger.info('🧠 Setting model to evaluation mode...')\\nmodel.eval()\\nlogger.info('✅ Model set to evaluation mode.')\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def listify_3d(x: dict):\n",
    "    logger.info('🧊 Listifying 3D dimensions...')\n",
    "    dimensions = [x['h'], x['w'], x['d']]\n",
    "    logger.debug(f'📏 Listified dimensions: {dimensions = }')\n",
    "    return dimensions\n",
    "\n",
    "logger.info('🎛️ Fetching hyperparameters...')\n",
    "hparams = model_cfg['hyperparameter']\n",
    "logger.debug(f'🎛️ Hyperparameters: {hparams = }')\n",
    "\n",
    "logger.info('🔗 Fetching label union...')\n",
    "union = model_cfg['data']['label_union']\n",
    "logger.debug(f'🔗 Label union: {union = }')\n",
    "\n",
    "logger.info('📐 Calculating ROI size...')\n",
    "roi_size = listify_3d(hparams['roi'])\n",
    "logger.debug(f'📐 ROI size: {roi_size = }')\n",
    "\n",
    "logger.info('🏗️ Creating model instance...')\n",
    "model = model.CustomSwinUNETR(\n",
    "    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\n",
    "    img_size          = roi_size,\n",
    "    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\n",
    "    feature_size      = hparams['feature_size'],\n",
    "    use_checkpoint    = True,\n",
    "    depths            = hparams['depths'],\n",
    "    num_heads         = hparams['num_heads'],\n",
    "    norm_name         = hparams['norm_name'],\n",
    "    normalize         = hparams['normalize'],\n",
    "    downsample        = hparams['downsample'],\n",
    "    use_v2            = hparams['use_v2'],\n",
    "    mlp_ratio         = hparams['mlp_ratio'],\n",
    "    qkv_bias          = hparams['qkv_bias'],\n",
    "    patch_size        = hparams['patch_size'],\n",
    "    window_size       = hparams['window_size'],\n",
    ")\n",
    "logger.info('✅ Model instance created.')\n",
    "\n",
    "logger.info('🔑 Fetching first model state key...')\n",
    "first_model_state_key = next(iter(model.state_dict().keys()))\n",
    "logger.debug(f'✅ First model state key: {first_model_state_key = }')\n",
    "\n",
    "logger.info('🔑 Fetching first state dict key...')\n",
    "first_state_dict_key = next(iter(state_dict.keys()))\n",
    "logger.debug(f'✅ First state dict key: {first_state_dict_key = }')\n",
    "\n",
    "logger.info('💾 Loading state dictionary into model...')\n",
    "model.load_state_dict(state_dict)\n",
    "logger.info('✅ State dictionary loaded.')\n",
    "\n",
    "logger.info('🖥️ Moving model to device...')\n",
    "model.to(device)\n",
    "logger.info('✅ Model moved to device.')\n",
    "\n",
    "logger.info('🧠 Setting model to evaluation mode...')\n",
    "model.eval()\n",
    "logger.info('✅ Model set to evaluation mode.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlZMp0kNJTn"
   },
   "source": [
    "# Data\n",
    "This section loads the data defined in your `config/config.yaml` file. Please see the example config file for more details. It requires you to have a nifti file for each MRI modality, typically T1, T2, T1-Contrast, and FLAIR, and the location to store the output nifti segmentation file. There are also instructions for loading multiple scans from a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFiR4fUuNJTo",
    "outputId": "d75fb267-4a8b-4974-91b8-6fc54694707d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,322 - brain_segmentation - INFO - 🔢 Setting up fold and configuration...\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - DEBUG - 📊 fold = 1\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - INFO - 🔍 Checking optional configuration keys...\n",
      "2025-09-23 03:11:01,323 - brain_segmentation - DEBUG - 🔍 Checking for key: compute_dice with value: True\n",
      "2025-09-23 03:11:01,324 - brain_segmentation - DEBUG - 🔍 Result of check: False\n",
      "2025-09-23 03:11:01,324 - brain_segmentation - DEBUG - 🎯 do_ground_truth = False\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - 🔍 Checking for key: mode with value: multi\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - 🔍 Result of check: True\n",
      "2025-09-23 03:11:01,325 - brain_segmentation - DEBUG - 📋 use_scan_list = True\n",
      "2025-09-23 03:11:01,326 - brain_segmentation - DEBUG - 📂 data_dir = '/mnt/d/A1_RainSun_20240916/1-UWMadison/IDiA-Lab/Tasks/tumor_seg/tumor_seg/data/Preprocessed'\n",
      "2025-09-23 03:11:01,326 - brain_segmentation - INFO - 📁 Loading scan data...\n",
      "2025-09-23 03:11:01,327 - brain_segmentation - INFO - 📚 Using multiple scans from JSON...\n",
      "2025-09-23 03:11:01,332 - brain_segmentation - INFO - 📏 Loading image size...\n",
      "2025-09-23 03:11:01,342 - brain_segmentation - DEBUG - 📐 Scan size: resize_shape = [240, 240, 155]\n",
      "2025-09-23 03:11:01,343 - brain_segmentation - INFO - 🔄 Setting up validation data loader...\n",
      "2025-09-23 03:11:01,345 - brain_segmentation - INFO - ✅ Validation data loader setup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 0\n",
      "Validation set size: 298\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Utility to safely extract optional boolean keys from nested config\n",
    "def check_optional_key(x: dict, key_name, true_val):\n",
    "    logger.debug(f'🔍 Checking for key: {key_name} with value: {true_val}')\n",
    "    result = (key_name in x.keys()) and (x[key_name] == true_val)\n",
    "    logger.debug(f'🔍 Result of check: {result}')\n",
    "    return result\n",
    "\n",
    "\n",
    "logger.info('🔢 Setting up fold and configuration...')\n",
    "fold = 1  # Let the validation fold be 1 - same convention as during training.\n",
    "logger.debug(f'📊 {fold = }')\n",
    "\n",
    "logger.info('🔍 Checking optional configuration keys...')\n",
    "do_ground_truth: bool = check_optional_key(inference_cfg['input'], 'compute_dice', True)\n",
    "logger.debug(f'🎯 {do_ground_truth = }')\n",
    "use_scan_list: bool = check_optional_key(inference_cfg['input'], 'mode', 'multi')\n",
    "logger.debug(f'📋 {use_scan_list = }')\n",
    "\n",
    "data_dir = inference_cfg['input']['data_dir']\n",
    "logger.debug(f'📂 {data_dir = }')\n",
    "\n",
    "logger.info('📁 Loading scan data...')\n",
    "if use_scan_list:\n",
    "    logger.info('📚 Using multiple scans from JSON...')\n",
    "    json_path = inference_cfg['input']['multi_scan']['scan_list']\n",
    "    _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "    with open(json_path) as f:\n",
    "        test_instance = json.load(f)['training'][0]['image'][2]  # To get image size, grab the T1 scan.\n",
    "else:\n",
    "    logger.info('🖼️ Using single scan...')\n",
    "    test_instance = inference_cfg['input']['single_scan']['t1']  # To get image size, grab the T1 scan.\n",
    "    json_data = {\n",
    "        'training': [\n",
    "            {\n",
    "                'fold': fold,\n",
    "                'image': [\n",
    "                    inference_cfg['input']['single_scan']['flair'],\n",
    "                    inference_cfg['input']['single_scan']['t1c'],\n",
    "                    inference_cfg['input']['single_scan']['t1'],\n",
    "                    inference_cfg['input']['single_scan']['t2']\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if do_ground_truth:\n",
    "        json_data['training'][0]['label'] = inference_cfg['input']['single_scan']['ground_truth']\n",
    "\n",
    "    logger.info('📝 Creating temporary JSON file...')\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:  # Use delete=False to fix a permissions error on Windows.\n",
    "        json.dump(json_data, temp_file, indent=4)\n",
    "        temp_file.flush()\n",
    "        json_path = temp_file.name\n",
    "        _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
    "        temp_file.close()\n",
    "        os.unlink(temp_file.name)\n",
    "    logger.info('✅ Temporary JSON file created and processed.')\n",
    "\n",
    "logger.info('📏 Loading image size...')\n",
    "resize_shape = list(nib.load(os.path.join(data_dir, test_instance)).shape)\n",
    "logger.debug(f'📐 Scan size: {resize_shape = }')\n",
    "\n",
    "logger.info('🔄 Setting up validation data loader...')\n",
    "val_loader = get_loader_val(\n",
    "    batch_size=1,\n",
    "    files=validation_files,\n",
    "    val_resize=resize_shape,\n",
    "    # union=union,\n",
    "    workers=1,\n",
    "    cache_dir='',\n",
    "    dataset_type='Dataset',\n",
    "    add_label=do_ground_truth,\n",
    ")\n",
    "logger.info('✅ Validation data loader setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_etOfVK4NJTo"
   },
   "source": [
    "# Inference\n",
    "This section takes the loaded model and runs inference on loaded scans, and saves the output to the location specified in your `config/config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dice score and output it\n",
    "import csv \n",
    "import os\n",
    "\n",
    "def save_dice_scores_to_csv(dice_scores, case_ids, output_csv_path):\n",
    "    \"\"\"\n",
    "    Save per-case Dice scores to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dice_scores (List[List[float]]): A list of [DICE_tc, DICE_wt, DICE_et, DICE_mean] for each case.\n",
    "        case_ids (List[str]): A list of case IDs corresponding to each Dice score row.\n",
    "        output_csv_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    with open(output_csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['CaseID', 'DICE_tc', 'DICE_wt', 'DICE_et', 'DICE_mean'])\n",
    "        for case_id, score in zip(case_ids, dice_scores):\n",
    "            writer.writerow([case_id] + score)\n",
    "\n",
    "    logger.info(f'📄 Dice scores saved to {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EH4JxkCUNJTo",
    "outputId": "c2c40d5c-e52d-44bc-e34d-01fe7d697f2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:11:01,358 - brain_segmentation - INFO - 📂 Output directory set to: ../outputs/output_0923_time1_03\n",
      "2025-09-23 03:11:01,367 - brain_segmentation - INFO - 🧮 Setting up accuracy function...\n",
      "2025-09-23 03:11:02,663 - brain_segmentation - INFO - 📥 Loading data for case 0...\n",
      "2025-09-23 03:11:02,665 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:11:02,673 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:12:00,617 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:12:00,720 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:12:00,782 - brain_segmentation - INFO - 🏷️ Label 0: 8877345 voxels\n",
      "2025-09-23 03:12:00,782 - brain_segmentation - INFO - 🏷️ Label 2: 23514 voxels\n",
      "2025-09-23 03:12:00,783 - brain_segmentation - INFO - 🏷️ Label 4: 27141 voxels\n",
      "2025-09-23 03:12:00,783 - brain_segmentation - INFO - 📊 Label 0: 99.43% of total volume\n",
      "2025-09-23 03:12:00,784 - brain_segmentation - INFO - 📊 Label 2: 0.26% of total volume\n",
      "2025-09-23 03:12:00,784 - brain_segmentation - INFO - 📊 Label 4: 0.30% of total volume\n",
      "2025-09-23 03:12:00,849 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100001_seg.nii.gz\n",
      "2025-09-23 03:12:00,850 - brain_segmentation - INFO - 📥 Loading data for case 1...\n",
      "2025-09-23 03:12:00,851 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:12:00,876 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:12:01,330 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:13:01,232 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:13:01,307 - brain_segmentation - INFO - 🏷️ Label 0: 8919210 voxels\n",
      "2025-09-23 03:13:01,307 - brain_segmentation - INFO - 🏷️ Label 2: 8790 voxels\n",
      "2025-09-23 03:13:01,308 - brain_segmentation - INFO - 📊 Label 0: 99.90% of total volume\n",
      "2025-09-23 03:13:01,308 - brain_segmentation - INFO - 📊 Label 2: 0.10% of total volume\n",
      "2025-09-23 03:13:01,359 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100002_seg.nii.gz\n",
      "2025-09-23 03:13:01,361 - brain_segmentation - INFO - 📥 Loading data for case 2...\n",
      "2025-09-23 03:13:01,361 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:13:01,375 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:13:01,830 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:14:01,965 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:14:02,023 - brain_segmentation - INFO - 🏷️ Label 0: 8842830 voxels\n",
      "2025-09-23 03:14:02,024 - brain_segmentation - INFO - 🏷️ Label 2: 74946 voxels\n",
      "2025-09-23 03:14:02,024 - brain_segmentation - INFO - 🏷️ Label 4: 10224 voxels\n",
      "2025-09-23 03:14:02,025 - brain_segmentation - INFO - 📊 Label 0: 99.05% of total volume\n",
      "2025-09-23 03:14:02,025 - brain_segmentation - INFO - 📊 Label 2: 0.84% of total volume\n",
      "2025-09-23 03:14:02,026 - brain_segmentation - INFO - 📊 Label 4: 0.11% of total volume\n",
      "2025-09-23 03:14:02,081 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100003_seg.nii.gz\n",
      "2025-09-23 03:14:02,082 - brain_segmentation - INFO - 📥 Loading data for case 3...\n",
      "2025-09-23 03:14:02,082 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:14:02,111 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:14:02,563 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:15:01,802 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:15:01,859 - brain_segmentation - INFO - 🏷️ Label 0: 8896378 voxels\n",
      "2025-09-23 03:15:01,860 - brain_segmentation - INFO - 🏷️ Label 2: 29722 voxels\n",
      "2025-09-23 03:15:01,860 - brain_segmentation - INFO - 🏷️ Label 4: 1900 voxels\n",
      "2025-09-23 03:15:01,861 - brain_segmentation - INFO - 📊 Label 0: 99.65% of total volume\n",
      "2025-09-23 03:15:01,861 - brain_segmentation - INFO - 📊 Label 2: 0.33% of total volume\n",
      "2025-09-23 03:15:01,862 - brain_segmentation - INFO - 📊 Label 4: 0.02% of total volume\n",
      "2025-09-23 03:15:01,920 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100004_seg.nii.gz\n",
      "2025-09-23 03:15:01,921 - brain_segmentation - INFO - 📥 Loading data for case 4...\n",
      "2025-09-23 03:15:01,922 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:15:01,944 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:15:02,401 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:16:01,679 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:16:01,735 - brain_segmentation - INFO - 🏷️ Label 0: 8906694 voxels\n",
      "2025-09-23 03:16:01,736 - brain_segmentation - INFO - 🏷️ Label 2: 6732 voxels\n",
      "2025-09-23 03:16:01,737 - brain_segmentation - INFO - 🏷️ Label 4: 14574 voxels\n",
      "2025-09-23 03:16:01,737 - brain_segmentation - INFO - 📊 Label 0: 99.76% of total volume\n",
      "2025-09-23 03:16:01,738 - brain_segmentation - INFO - 📊 Label 2: 0.08% of total volume\n",
      "2025-09-23 03:16:01,738 - brain_segmentation - INFO - 📊 Label 4: 0.16% of total volume\n",
      "2025-09-23 03:16:01,789 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100005_seg.nii.gz\n",
      "2025-09-23 03:16:01,790 - brain_segmentation - INFO - 📥 Loading data for case 5...\n",
      "2025-09-23 03:16:01,790 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:16:01,803 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:16:02,255 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:17:01,414 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:17:01,470 - brain_segmentation - INFO - 🏷️ Label 0: 8865193 voxels\n",
      "2025-09-23 03:17:01,470 - brain_segmentation - INFO - 🏷️ Label 2: 55836 voxels\n",
      "2025-09-23 03:17:01,471 - brain_segmentation - INFO - 🏷️ Label 4: 6971 voxels\n",
      "2025-09-23 03:17:01,471 - brain_segmentation - INFO - 📊 Label 0: 99.30% of total volume\n",
      "2025-09-23 03:17:01,472 - brain_segmentation - INFO - 📊 Label 2: 0.63% of total volume\n",
      "2025-09-23 03:17:01,472 - brain_segmentation - INFO - 📊 Label 4: 0.08% of total volume\n",
      "2025-09-23 03:17:01,521 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100006_seg.nii.gz\n",
      "2025-09-23 03:17:01,522 - brain_segmentation - INFO - 📥 Loading data for case 6...\n",
      "2025-09-23 03:17:01,523 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:17:01,545 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:17:01,998 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:18:01,284 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:18:01,345 - brain_segmentation - INFO - 🏷️ Label 0: 8925092 voxels\n",
      "2025-09-23 03:18:01,346 - brain_segmentation - INFO - 🏷️ Label 2: 2727 voxels\n",
      "2025-09-23 03:18:01,346 - brain_segmentation - INFO - 🏷️ Label 4: 181 voxels\n",
      "2025-09-23 03:18:01,347 - brain_segmentation - INFO - 📊 Label 0: 99.97% of total volume\n",
      "2025-09-23 03:18:01,347 - brain_segmentation - INFO - 📊 Label 2: 0.03% of total volume\n",
      "2025-09-23 03:18:01,348 - brain_segmentation - INFO - 📊 Label 4: 0.00% of total volume\n",
      "2025-09-23 03:18:01,391 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100007_seg.nii.gz\n",
      "2025-09-23 03:18:01,392 - brain_segmentation - INFO - 📥 Loading data for case 7...\n",
      "2025-09-23 03:18:01,393 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:18:01,441 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:18:01,898 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:19:01,110 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:19:01,174 - brain_segmentation - INFO - 🏷️ Label 0: 8869969 voxels\n",
      "2025-09-23 03:19:01,175 - brain_segmentation - INFO - 🏷️ Label 2: 52143 voxels\n",
      "2025-09-23 03:19:01,175 - brain_segmentation - INFO - 🏷️ Label 4: 5888 voxels\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - 📊 Label 0: 99.35% of total volume\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - 📊 Label 2: 0.58% of total volume\n",
      "2025-09-23 03:19:01,176 - brain_segmentation - INFO - 📊 Label 4: 0.07% of total volume\n",
      "2025-09-23 03:19:01,237 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100008_seg.nii.gz\n",
      "2025-09-23 03:19:01,238 - brain_segmentation - INFO - 📥 Loading data for case 8...\n",
      "2025-09-23 03:19:01,239 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:19:01,263 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:19:01,720 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:20:00,885 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:20:00,943 - brain_segmentation - INFO - 🏷️ Label 0: 8845751 voxels\n",
      "2025-09-23 03:20:00,944 - brain_segmentation - INFO - 🏷️ Label 2: 44646 voxels\n",
      "2025-09-23 03:20:00,944 - brain_segmentation - INFO - 🏷️ Label 4: 37603 voxels\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - 📊 Label 0: 99.08% of total volume\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - 📊 Label 2: 0.50% of total volume\n",
      "2025-09-23 03:20:00,945 - brain_segmentation - INFO - 📊 Label 4: 0.42% of total volume\n",
      "2025-09-23 03:20:01,014 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100009_seg.nii.gz\n",
      "2025-09-23 03:20:01,015 - brain_segmentation - INFO - 📥 Loading data for case 9...\n",
      "2025-09-23 03:20:01,015 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:20:01,038 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:20:01,497 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:21:00,674 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:21:00,732 - brain_segmentation - INFO - 🏷️ Label 0: 8837814 voxels\n",
      "2025-09-23 03:21:00,732 - brain_segmentation - INFO - 🏷️ Label 2: 68009 voxels\n",
      "2025-09-23 03:21:00,733 - brain_segmentation - INFO - 🏷️ Label 4: 22177 voxels\n",
      "2025-09-23 03:21:00,734 - brain_segmentation - INFO - 📊 Label 0: 98.99% of total volume\n",
      "2025-09-23 03:21:00,734 - brain_segmentation - INFO - 📊 Label 2: 0.76% of total volume\n",
      "2025-09-23 03:21:00,736 - brain_segmentation - INFO - 📊 Label 4: 0.25% of total volume\n",
      "2025-09-23 03:21:00,790 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100010_seg.nii.gz\n",
      "2025-09-23 03:21:00,791 - brain_segmentation - INFO - 📥 Loading data for case 10...\n",
      "2025-09-23 03:21:00,792 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:21:00,811 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:21:01,266 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:22:00,522 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:22:00,580 - brain_segmentation - INFO - 🏷️ Label 0: 8913503 voxels\n",
      "2025-09-23 03:22:00,580 - brain_segmentation - INFO - 🏷️ Label 2: 14089 voxels\n",
      "2025-09-23 03:22:00,581 - brain_segmentation - INFO - 🏷️ Label 4: 408 voxels\n",
      "2025-09-23 03:22:00,581 - brain_segmentation - INFO - 📊 Label 0: 99.84% of total volume\n",
      "2025-09-23 03:22:00,582 - brain_segmentation - INFO - 📊 Label 2: 0.16% of total volume\n",
      "2025-09-23 03:22:00,582 - brain_segmentation - INFO - 📊 Label 4: 0.00% of total volume\n",
      "2025-09-23 03:22:00,636 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100011_seg.nii.gz\n",
      "2025-09-23 03:22:00,637 - brain_segmentation - INFO - 📥 Loading data for case 11...\n",
      "2025-09-23 03:22:00,637 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:22:00,663 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:22:01,116 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:23:00,326 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:23:00,388 - brain_segmentation - INFO - 🏷️ Label 0: 8899224 voxels\n",
      "2025-09-23 03:23:00,388 - brain_segmentation - INFO - 🏷️ Label 2: 14750 voxels\n",
      "2025-09-23 03:23:00,389 - brain_segmentation - INFO - 🏷️ Label 4: 14026 voxels\n",
      "2025-09-23 03:23:00,389 - brain_segmentation - INFO - 📊 Label 0: 99.68% of total volume\n",
      "2025-09-23 03:23:00,390 - brain_segmentation - INFO - 📊 Label 2: 0.17% of total volume\n",
      "2025-09-23 03:23:00,390 - brain_segmentation - INFO - 📊 Label 4: 0.16% of total volume\n",
      "2025-09-23 03:23:00,450 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100012_seg.nii.gz\n",
      "2025-09-23 03:23:00,451 - brain_segmentation - INFO - 📥 Loading data for case 12...\n",
      "2025-09-23 03:23:00,452 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:23:00,493 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:23:00,947 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:24:00,271 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:24:00,331 - brain_segmentation - INFO - 🏷️ Label 0: 8906254 voxels\n",
      "2025-09-23 03:24:00,332 - brain_segmentation - INFO - 🏷️ Label 2: 16150 voxels\n",
      "2025-09-23 03:24:00,332 - brain_segmentation - INFO - 🏷️ Label 4: 5596 voxels\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - 📊 Label 0: 99.76% of total volume\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - 📊 Label 2: 0.18% of total volume\n",
      "2025-09-23 03:24:00,333 - brain_segmentation - INFO - 📊 Label 4: 0.06% of total volume\n",
      "2025-09-23 03:24:00,391 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100013_seg.nii.gz\n",
      "2025-09-23 03:24:00,392 - brain_segmentation - INFO - 📥 Loading data for case 13...\n",
      "2025-09-23 03:24:00,393 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:24:00,415 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:24:00,870 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:00,110 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:25:00,171 - brain_segmentation - INFO - 🏷️ Label 0: 8876548 voxels\n",
      "2025-09-23 03:25:00,171 - brain_segmentation - INFO - 🏷️ Label 2: 48766 voxels\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - 🏷️ Label 4: 2686 voxels\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - 📊 Label 0: 99.42% of total volume\n",
      "2025-09-23 03:25:00,172 - brain_segmentation - INFO - 📊 Label 2: 0.55% of total volume\n",
      "2025-09-23 03:25:00,173 - brain_segmentation - INFO - 📊 Label 4: 0.03% of total volume\n",
      "2025-09-23 03:25:00,215 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100014_seg.nii.gz\n",
      "2025-09-23 03:25:00,217 - brain_segmentation - INFO - 📥 Loading data for case 14...\n",
      "2025-09-23 03:25:00,218 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:00,232 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:25:00,688 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:25:59,948 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:26:00,009 - brain_segmentation - INFO - 🏷️ Label 0: 8859275 voxels\n",
      "2025-09-23 03:26:00,010 - brain_segmentation - INFO - 🏷️ Label 2: 66315 voxels\n",
      "2025-09-23 03:26:00,010 - brain_segmentation - INFO - 🏷️ Label 4: 2410 voxels\n",
      "2025-09-23 03:26:00,011 - brain_segmentation - INFO - 📊 Label 0: 99.23% of total volume\n",
      "2025-09-23 03:26:00,011 - brain_segmentation - INFO - 📊 Label 2: 0.74% of total volume\n",
      "2025-09-23 03:26:00,012 - brain_segmentation - INFO - 📊 Label 4: 0.03% of total volume\n",
      "2025-09-23 03:26:00,060 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100015_seg.nii.gz\n",
      "2025-09-23 03:26:00,060 - brain_segmentation - INFO - 📥 Loading data for case 15...\n",
      "2025-09-23 03:26:00,061 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:00,082 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:26:00,541 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:59,719 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:26:59,778 - brain_segmentation - INFO - 🏷️ Label 0: 8919303 voxels\n",
      "2025-09-23 03:26:59,779 - brain_segmentation - INFO - 🏷️ Label 2: 6544 voxels\n",
      "2025-09-23 03:26:59,779 - brain_segmentation - INFO - 🏷️ Label 4: 2153 voxels\n",
      "2025-09-23 03:26:59,780 - brain_segmentation - INFO - 📊 Label 0: 99.90% of total volume\n",
      "2025-09-23 03:26:59,780 - brain_segmentation - INFO - 📊 Label 2: 0.07% of total volume\n",
      "2025-09-23 03:26:59,781 - brain_segmentation - INFO - 📊 Label 4: 0.02% of total volume\n",
      "2025-09-23 03:26:59,838 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100016_seg.nii.gz\n",
      "2025-09-23 03:26:59,839 - brain_segmentation - INFO - 📥 Loading data for case 16...\n",
      "2025-09-23 03:26:59,840 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:26:59,857 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:27:00,318 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:27:59,460 - brain_segmentation - INFO - 🧠 Generating segmentation mask...\n",
      "2025-09-23 03:27:59,520 - brain_segmentation - INFO - 🏷️ Label 0: 8916354 voxels\n",
      "2025-09-23 03:27:59,521 - brain_segmentation - INFO - 🏷️ Label 2: 10016 voxels\n",
      "2025-09-23 03:27:59,521 - brain_segmentation - INFO - 🏷️ Label 4: 1630 voxels\n",
      "2025-09-23 03:27:59,522 - brain_segmentation - INFO - 📊 Label 0: 99.87% of total volume\n",
      "2025-09-23 03:27:59,522 - brain_segmentation - INFO - 📊 Label 2: 0.11% of total volume\n",
      "2025-09-23 03:27:59,523 - brain_segmentation - INFO - 📊 Label 4: 0.02% of total volume\n",
      "2025-09-23 03:27:59,562 - brain_segmentation - INFO - 💾 Saved segmentation to: ../outputs/output_0923_time1_03/100017_seg.nii.gz\n",
      "2025-09-23 03:27:59,563 - brain_segmentation - INFO - 📥 Loading data for case 17...\n",
      "2025-09-23 03:27:59,563 - brain_segmentation - DEBUG - 🖼️ val_images.shape = torch.Size([1, 4, 240, 240, 155])\n",
      "2025-09-23 03:27:59,575 - brain_segmentation - DEBUG - 📐 affine.shape = (4, 4)\n",
      "2025-09-23 03:28:00,028 - brain_segmentation - DEBUG - 🔢 val_outputs.shape = torch.Size([1, 4, 240, 240, 155])\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"../outputs/output_0923_time1_03\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"📂 Output directory set to: {output_dir}\")\n",
    "\n",
    "case_ids = []\n",
    "dice_scores = []\n",
    "\n",
    "# 🔁 If using scan list, load and filter it once outside the loop\n",
    "if use_scan_list:\n",
    "    with open(json_path) as f:\n",
    "        d = json.load(f)\n",
    "    scan_entries = [entry for entry in d['training'] if entry['fold'] == fold]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logger.info('🧮 Setting up accuracy function...')\n",
    "    acc_func = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH)\n",
    "\n",
    "    loop_data = zip(val_loader, scan_entries) if use_scan_list else enumerate(val_loader)\n",
    "\n",
    "    for i, data in enumerate(loop_data):\n",
    "        val_data = data[0]\n",
    "        scan_entry = data[1] if use_scan_list else None\n",
    "\n",
    "        logger.info(f'📥 Loading data for case {i}...')\n",
    "        val_images = val_data['image']\n",
    "        logger.debug(f'🖼️ val_images.shape = {val_images.shape}')\n",
    "\n",
    "        # 🔍 Load affine matrix\n",
    "        if use_scan_list:\n",
    "            affine_path = os.path.join(data_dir, scan_entry['image'][2])\n",
    "            case_id = os.path.basename(os.path.dirname(affine_path))\n",
    "        else:\n",
    "            affine_path = os.path.join(data_dir, inference_cfg['input']['single_scan']['t1'])\n",
    "            case_id = 'single_scan'\n",
    "\n",
    "        affine = nib.load(affine_path).affine\n",
    "        logger.debug(f'📐 affine.shape = {affine.shape}')\n",
    "\n",
    "        # 🔄 Perform inference\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_images.to(device),\n",
    "            roi_size=roi_size,\n",
    "            sw_batch_size=4,\n",
    "            predictor=model_instance,\n",
    "        )\n",
    "        logger.debug(f'🔢 val_outputs.shape = {val_outputs.shape}')\n",
    "\n",
    "        # 💡 Post-processing predictions\n",
    "        val_outputs = val_outputs.cpu()[0]  # remove batch dim: [C, H, W, D]\n",
    "        val_outputs_per_channel = torch.unbind(val_outputs, dim=0)\n",
    "        post_sigmoid = Activations(sigmoid=True)\n",
    "        post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "        val_outputs_convert = [post_pred(post_sigmoid(c)) for c in val_outputs_per_channel]\n",
    "\n",
    "        # 🧮 DICE calculation\n",
    "        if do_ground_truth:\n",
    "            logger.info('📊 Calculating DICE scores...')\n",
    "            ground_truth = val_data['label']\n",
    "            acc_func.reset()\n",
    "            val_outputs_used = val_outputs_convert[:-1] if len(val_outputs_convert) > 3 else val_outputs_convert\n",
    "            val_outputs_used = [v.to(device) for v in val_outputs_used]\n",
    "            y_pred_tensor = torch.stack(val_outputs_used, dim=0).unsqueeze(0)  # shape [1, C, H, W, D]\n",
    "            acc_func(y_pred=y_pred_tensor, y=ground_truth.to(device))\n",
    "            acc = acc_func.aggregate().cpu().numpy()\n",
    "\n",
    "            num_zeroes = list(acc[:3]).count(0.0)\n",
    "            mean = (acc[0] + acc[1] + acc[2]) / (3 - num_zeroes) if num_zeroes < 3 else 0\n",
    "            logger.info(f'📊 DICE (tc): {acc[0]}')\n",
    "            logger.info(f'📊 DICE (wt): {acc[1]}')\n",
    "            logger.info(f'📊 DICE (et): {acc[2]}')\n",
    "            logger.info(f'📊 DICE (mean): {mean}')\n",
    "\n",
    "            dice_scores.append([acc[0], acc[1], acc[2], mean])\n",
    "            case_ids.append(case_id)\n",
    "\n",
    "        # 🧠 Segmentation mask generation (channel-wise, no argmax)\n",
    "        logger.info('🧠 Generating segmentation mask...')\n",
    "        segmentation_mask = val_outputs_convert[:-1] if len(val_outputs_convert) > 3 else val_outputs_convert\n",
    "\n",
    "        # You can update this map based on what you want each channel to represent\n",
    "        channel_label_map = {0: 0, 1: 2, 2: 4, 3: 3, 4:1}  # e.g. 1 = NCR, 2 = SNFH, 3 = ET\n",
    "\n",
    "        remapped = np.zeros_like(segmentation_mask[0].cpu().numpy(), dtype=np.uint8)\n",
    "        for i, mask in enumerate(segmentation_mask):\n",
    "            binary_mask = mask.cpu().numpy().astype(bool)\n",
    "            if i in channel_label_map:\n",
    "                remapped[binary_mask] = channel_label_map[i]\n",
    "\n",
    "        segmentation_mask = remapped\n",
    "\n",
    "        # 🧾 Log label distribution\n",
    "        unique_labels, label_counts = np.unique(segmentation_mask, return_counts=True)\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            logger.info(f'🏷️ Label {int(label)}: {count} voxels')\n",
    "\n",
    "        total_voxels = np.prod(segmentation_mask.shape)\n",
    "        for label, count in zip(unique_labels, label_counts):\n",
    "            percentage = (count / total_voxels) * 100\n",
    "            logger.info(f'📊 Label {int(label)}: {percentage:.2f}% of total volume')\n",
    "\n",
    "        # 💾 Save NIfTI\n",
    "        save_pth = os.path.join(output_dir, f'{case_id}_seg.nii.gz')\n",
    "        nib.save(nib.Nifti1Image(segmentation_mask.astype(np.uint8), affine=affine), save_pth)\n",
    "        logger.info(f'💾 Saved segmentation to: {save_pth}')\n",
    "\n",
    "# ✅ Final Dice summary\n",
    "if do_ground_truth and dice_scores:\n",
    "    logger.info('📊 Calculating mean DICE scores...')\n",
    "    mask = np.ma.masked_equal(dice_scores, 0)\n",
    "    mean = mask.mean(axis=0).filled(np.nan)\n",
    "    logger.info(f'📊 DICE (tc): {mean[0]}')\n",
    "    logger.info(f'📊 DICE (wt): {mean[1]}')\n",
    "    logger.info(f'📊 DICE (et): {mean[2]}')\n",
    "    logger.info(f'📊 DICE (mean): {mean[3]}')\n",
    "\n",
    "    # 💾 Save per-case DICE to CSV\n",
    "    logger.info(\"💾 Saving per-case DICE scores to CSV...\")\n",
    "    dice_df = pd.DataFrame(dice_scores, columns=[\"DICE_tc\", \"DICE_wt\", \"DICE_et\", \"DICE_mean\"])\n",
    "    dice_df.insert(0, \"Case_ID\", case_ids)\n",
    "    dice_df.to_csv(os.path.join(output_dir, \"dice_scores.csv\"), index=False)\n",
    "    logger.info(f\"📄 Saved per-case DICE scores to: {os.path.join(output_dir, 'dice_scores.csv')}\")\n",
    "\n",
    "    # 💾 Save mean DICE score to CSV\n",
    "    mean_df = pd.DataFrame([{\n",
    "        \"DICE_tc\": mean[0],\n",
    "        \"DICE_wt\": mean[1],\n",
    "        \"DICE_et\": mean[2],\n",
    "        \"DICE_mean\": mean[3],\n",
    "    }])\n",
    "    mean_df.to_csv(os.path.join(output_dir, \"mean_dice_score.csv\"), index=False)\n",
    "    logger.info(f\"📄 Saved mean DICE score to: {os.path.join(output_dir, 'mean_dice_score.csv')}\")\n",
    "\n",
    "logger.info('✅ Inference complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, mask in enumerate(segmentation_mask):\n",
    "#     print(f\"Element {i} type: {type(mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mask in enumerate(segmentation_mask):\n",
    "    assert mask.shape == torch.Size([240, 240, 155]), f\"Shape mismatch at index {i}: {mask.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask_sum = torch.sum(torch.stack(segmentation_mask, dim=0), dim=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
