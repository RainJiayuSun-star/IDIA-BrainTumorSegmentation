{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9O2FMEC9OeU"
      },
      "source": [
        "# Overview\n",
        "**BRAIN SEGMENTATION INFERENCE**\n",
        "This Jupyter notebook is designed to run inference on a brain MRI scan using a pre-trained segmentation model. It downloads a model from the cloud (requiring AWS access keys to be set up properly) and then runs inference on a single scan. The output is a nifti file with the segmentation labels. You may wish to then load it in a program such as Slicer to view the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjoOMwOTNJTd"
      },
      "source": [
        "# Setup\n",
        "First, set up a python environment with the necessary packages.\n",
        "\n",
        "For instance, in VSCode or Cursor, press Ctrl+Shift+P and type \"Python: Create Environment\" and follow the prompts.\n",
        "\n",
        "Or, on the command line, run:\n",
        "```bash\n",
        "python -m venv brain_segmentation_inference_env\n",
        "```\n",
        "--or--\n",
        "```bash\n",
        "python3 -m venv brain_segmentation_inference_env\n",
        "```\n",
        "and then either \n",
        "```bash\n",
        "brain_segmentation_inference_env\\Scripts\\activate\n",
        "```\n",
        "on Windows, or\n",
        "```bash\n",
        "source brain_segmentation_inference_env/bin/activate\n",
        "```\n",
        "on macOS and Linux.\n",
        "\n",
        "Then, install the necessary packages:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "After all that, this cell should run without error and import all the necessary packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Create a named logger\n",
        "logger = logging.getLogger('brain_segmentation')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a console handler and set its level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Add the formatter to the console handler\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the console handler to the logger\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Turn off logs from all other loggers\n",
        "for name in logging.root.manager.loggerDict:\n",
        "    if name != 'brain_segmentation':\n",
        "        logging.getLogger(name).setLevel(logging.CRITICAL)\n",
        "\n",
        "logger.info('ğŸš€ Setting up logging...')\n",
        "logger.debug(f'ğŸ”§ Current logging level: {logger.getEffectiveLevel()}')\n",
        "logger.info('âœ… Logging setup complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdAAZeqmPNsv",
        "outputId": "2706f96f-1b8d-4f0d-9f3e-b0f3c8dbaa2f"
      },
      "outputs": [],
      "source": [
        "logger.info('ğŸ“¦ Importing outside packages...')\n",
        "import os\n",
        "import torch\n",
        "import json, yaml\n",
        "import tempfile\n",
        "import boto3\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "from smart_open import open\n",
        "from monai.transforms import AsDiscrete, Activations\n",
        "from monai.utils.enums import MetricReduction\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.inferers import sliding_window_inference\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "logger.info('âœ… Outside packages imported.')\n",
        "\n",
        "logger.info('ğŸ“¦ Importing local packages...')\n",
        "from core_common import get_loader_val, datafold_read\n",
        "import custom_model\n",
        "logger.info('âœ… Local packages imported.')\n",
        "\n",
        "logger.info('ğŸ“¦ Setting up device...')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info('âœ… Device set up.')\n",
        "logger.debug(f'ğŸ–¥ï¸ {device = }')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OeN_BW_NJTj"
      },
      "source": [
        "# Model\n",
        "This part loads the model from the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5dkAr0NJTl"
      },
      "source": [
        "## Download\n",
        "This part downloads the model from the cloud.\n",
        "\n",
        "You'll need to have an AWS profile named 'theta-model-downloader'. You can create this profile by running this command in your terminal:\n",
        "```bash\n",
        "aws configure --profile theta-model-downloader\n",
        "```\n",
        "and entering your AWS credentials.\n",
        "\n",
        "If you do not have the AWS command line tools installed, you can install them by following the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb1qCskWNJTm",
        "outputId": "a8a36564-66cf-4cf1-c8e9-b7201c6f6b93"
      },
      "outputs": [],
      "source": [
        "logger.info('ğŸ“‚ Loading configuration file...')\n",
        "config_pth = 'config/config.yaml'\n",
        "\n",
        "logger.info('ğŸ”“ Opening configuration file...')\n",
        "with open(config_pth, 'r') as file:\n",
        "    logger.info('ğŸ“– Reading YAML content...')\n",
        "    inference_cfg = yaml.safe_load(file)\n",
        "logger.debug(f'ğŸ”§ {inference_cfg = }')\n",
        "logger.info('âœ… Configuration loaded.')\n",
        "\n",
        "logger.info('ğŸ”§ Defining utility function...')\n",
        "def check_optional_key(x: dict, key_name, true_val):\n",
        "    logger.debug(f'ğŸ” Checking for key: {key_name} with value: {true_val}')\n",
        "    result = (key_name in x.keys()) and (x[key_name] == true_val)\n",
        "    logger.debug(f'ğŸ” Result of check: {result}')\n",
        "    return result\n",
        "logger.info('âœ… Utility function defined.')\n",
        "\n",
        "logger.info('â˜ï¸ Setting up AWS S3 connection...')\n",
        "bucket = inference_cfg['model']['bucket']\n",
        "key = inference_cfg['model']['key']\n",
        "logger.info('ğŸ” Creating AWS session...')\n",
        "session = boto3.Session(\n",
        "    profile_name='theta-model-downloader',\n",
        "    region_name=inference_cfg['model']['region'])\n",
        "logger.info('ğŸ”— Creating S3 client...')\n",
        "s3_client = session.client('s3')\n",
        "logger.debug(f'ğŸª£ {bucket = }')\n",
        "logger.debug(f'ğŸ”‘ {key = }')\n",
        "logger.info('âœ… AWS S3 connection set up.')\n",
        "\n",
        "logger.info('ğŸ“¦ Fetching model metadata...')\n",
        "logger.info('ğŸ” Retrieving object metadata from S3...')\n",
        "metadata = s3_client.head_object(Bucket=bucket, Key=key)\n",
        "file_size = metadata['ContentLength']\n",
        "logger.info('ğŸ”§ Parsing model configuration from metadata...')\n",
        "model_cfg = yaml.safe_load(metadata['Metadata']['cfg'])\n",
        "logger.debug(f'ğŸ“ {file_size = }')\n",
        "logger.debug(f'ğŸ”§ {model_cfg = }')\n",
        "logger.info('âœ… Model metadata fetched.')\n",
        "\n",
        "logger.info('â¬‡ï¸ Downloading model checkpoint...')\n",
        "logger.info('ğŸ“ Creating temporary file...')\n",
        "with tempfile.NamedTemporaryFile(mode='wb', suffix='.ckpt', delete=False) as temp_file:\n",
        "    logger.info('ğŸ”„ Setting up progress bar...')\n",
        "    with tqdm(total=file_size, unit='B', unit_scale=True, desc='Downloading checkpoint...') as progress_bar:\n",
        "        def update_progress(chunk):\n",
        "            progress_bar.update(chunk)\n",
        "        logger.info('ğŸ”„ Starting file download...')\n",
        "        s3_client.download_fileobj(Bucket=bucket, Key=key, Fileobj=temp_file, Callback=update_progress)\n",
        "\n",
        "    logger.info('ğŸ’¾ Flushing temporary file...')\n",
        "    temp_file.flush()\n",
        "    logger.info('ğŸ”„ Loading checkpoint into memory...')\n",
        "    checkpoint = torch.load(temp_file.name, map_location=device)\n",
        "    logger.info('ğŸ—‘ï¸ Closing and removing temporary file...')\n",
        "    temp_file.close()\n",
        "    os.unlink(temp_file.name) \n",
        "logger.info('âœ… Model checkpoint downloaded and loaded.')\n",
        "\n",
        "logger.info('ğŸ”„ Processing state dictionary...')\n",
        "state_dict = checkpoint['state_dict']\n",
        "logger.info('ğŸ”„ Removing \"model.\" prefix from state dict keys...')\n",
        "state_dict = {key.replace('model.', ''): value for key, value in state_dict.items()}\n",
        "logger.debug(f'ğŸ”‘ State dict keys: {state_dict.keys()}')\n",
        "logger.info('âœ… State dictionary processed.')\n",
        "\n",
        "logger.info('ğŸ”„ Printing model configuration...')\n",
        "pprint.pprint(model_cfg)\n",
        "logger.info('âœ… Model configuration printed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9E4uJj9NJTn"
      },
      "source": [
        "## Load\n",
        "Once the model has been downloaded, this section loads the model into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujk2UMcINJTn",
        "outputId": "e29fca60-dc4b-4df8-b32d-d9f147c95a75"
      },
      "outputs": [],
      "source": [
        "def listify_3d(x: dict):\n",
        "    logger.info('ğŸ§Š Listifying 3D dimensions...')\n",
        "    dimensions = [x['h'], x['w'], x['d']]\n",
        "    logger.debug(f'ğŸ“ Listified dimensions: {dimensions = }')\n",
        "    return dimensions\n",
        "\n",
        "logger.info('ğŸ›ï¸ Fetching hyperparameters...')\n",
        "hparams = model_cfg['hyperparameter']\n",
        "logger.debug(f'ğŸ›ï¸ Hyperparameters: {hparams = }')\n",
        "\n",
        "logger.info('ğŸ”— Fetching label union...')\n",
        "union = model_cfg['data']['label_union']\n",
        "logger.debug(f'ğŸ”— Label union: {union = }')\n",
        "\n",
        "logger.info('ğŸ“ Calculating ROI size...')\n",
        "roi_size = listify_3d(hparams['roi'])\n",
        "logger.debug(f'ğŸ“ ROI size: {roi_size = }')\n",
        "\n",
        "logger.info('ğŸ—ï¸ Creating model instance...')\n",
        "model = custom_model.CustomSwinUNETR(\n",
        "    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\n",
        "    img_size          = roi_size,\n",
        "    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\n",
        "    feature_size      = hparams['feature_size'],\n",
        "    use_checkpoint    = True,\n",
        "    depths            = hparams['depths'],\n",
        "    num_heads         = hparams['num_heads'],\n",
        "    norm_name         = hparams['norm_name'],\n",
        "    normalize         = hparams['normalize'],\n",
        "    downsample        = hparams['downsample'],\n",
        "    use_v2            = hparams['use_v2'],\n",
        "    mlp_ratio         = hparams['mlp_ratio'],\n",
        "    qkv_bias          = hparams['qkv_bias'],\n",
        "    patch_size        = hparams['patch_size'],\n",
        "    window_size       = hparams['window_size'],\n",
        ")\n",
        "logger.info('âœ… Model instance created.')\n",
        "\n",
        "logger.info('ğŸ”‘ Fetching first model state key...')\n",
        "first_model_state_key = next(iter(model.state_dict().keys()))\n",
        "logger.debug(f'âœ… First model state key: {first_model_state_key = }')\n",
        "\n",
        "logger.info('ğŸ”‘ Fetching first state dict key...')\n",
        "first_state_dict_key = next(iter(state_dict.keys()))\n",
        "logger.debug(f'âœ… First state dict key: {first_state_dict_key = }')\n",
        "\n",
        "logger.info('ğŸ’¾ Loading state dictionary into model...')\n",
        "model.load_state_dict(state_dict)\n",
        "logger.info('âœ… State dictionary loaded.')\n",
        "\n",
        "logger.info('ğŸ–¥ï¸ Moving model to device...')\n",
        "model.to(device)\n",
        "logger.info('âœ… Model moved to device.')\n",
        "\n",
        "logger.info('ğŸ§  Setting model to evaluation mode...')\n",
        "model.eval()\n",
        "logger.info('âœ… Model set to evaluation mode.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYlZMp0kNJTn"
      },
      "source": [
        "# Data\n",
        "This section loads the data defined in your `config/config.yaml` file. Please see the example config file for more details. It requires you to have a nifti file for each MRI modality, typically T1, T2, T1-Contrast, and FLAIR, and the location to store the output nifti segmentation file. There are also instructions for loading multiple scans from a json file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFiR4fUuNJTo",
        "outputId": "d75fb267-4a8b-4974-91b8-6fc54694707d"
      },
      "outputs": [],
      "source": [
        "logger.info('ğŸ”¢ Setting up fold and configuration...')\n",
        "fold = 1  # Let the validation fold be 1 - same convention as during training.\n",
        "logger.debug(f'ğŸ“Š {fold = }')\n",
        "\n",
        "logger.info('ğŸ” Checking optional configuration keys...')\n",
        "do_ground_truth: bool = check_optional_key(inference_cfg['input'], 'compute_dice', True)\n",
        "logger.debug(f'ğŸ¯ {do_ground_truth = }')\n",
        "use_scan_list: bool = check_optional_key(inference_cfg['input'], 'mode', 'multi')\n",
        "logger.debug(f'ğŸ“‹ {use_scan_list = }')\n",
        "\n",
        "data_dir = inference_cfg['input']['data_dir']\n",
        "logger.debug(f'ğŸ“‚ {data_dir = }')\n",
        "\n",
        "logger.info('ğŸ“ Loading scan data...')\n",
        "if use_scan_list:\n",
        "    logger.info('ğŸ“š Using multiple scans from JSON...')\n",
        "    json_path = inference_cfg['input']['multi_scan']['scan_list']\n",
        "    _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
        "    with open(json_path) as f:\n",
        "        test_instance = json.load(f)['training'][0]['image'][2]  # To get image size, grab the T1 scan.\n",
        "else:\n",
        "    logger.info('ğŸ–¼ï¸ Using single scan...')\n",
        "    test_instance = inference_cfg['input']['single_scan']['t1']  # To get image size, grab the T1 scan.\n",
        "    json_data = {\n",
        "        'training': [\n",
        "            {\n",
        "                'fold': fold,\n",
        "                'image': [\n",
        "                    inference_cfg['input']['single_scan']['flair'],\n",
        "                    inference_cfg['input']['single_scan']['t1c'],\n",
        "                    inference_cfg['input']['single_scan']['t1'],\n",
        "                    inference_cfg['input']['single_scan']['t2']\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    if do_ground_truth:\n",
        "        json_data['training'][0]['label'] = inference_cfg['input']['single_scan']['ground_truth']\n",
        "\n",
        "    logger.info('ğŸ“ Creating temporary JSON file...')\n",
        "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:  # Use delete=False to fix a permissions error on Windows.\n",
        "        json.dump(json_data, temp_file, indent=4)\n",
        "        temp_file.flush()\n",
        "        json_path = temp_file.name\n",
        "        _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
        "        temp_file.close()\n",
        "        os.unlink(temp_file.name)\n",
        "    logger.info('âœ… Temporary JSON file created and processed.')\n",
        "\n",
        "logger.info('ğŸ“ Loading image size...')\n",
        "resize_shape = list(nib.load(os.path.join(data_dir, test_instance)).shape)\n",
        "logger.debug(f'ğŸ“ Scan size: {resize_shape = }')\n",
        "\n",
        "logger.info('ğŸ”„ Setting up validation data loader...')\n",
        "val_loader = get_loader_val(\n",
        "    batch_size=1,\n",
        "    files=validation_files,\n",
        "    val_resize=resize_shape,\n",
        "    union=union,\n",
        "    workers=1,\n",
        "    cache_dir='',\n",
        "    dataset_type='Dataset',\n",
        "    add_label=do_ground_truth,\n",
        ")\n",
        "logger.info('âœ… Validation data loader setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_etOfVK4NJTo"
      },
      "source": [
        "# Inference\n",
        "This section takes the loaded model and runs inference on loaded scans, and saves the output to the location specified in your `config/config.yaml` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4JxkCUNJTo",
        "outputId": "c2c40d5c-e52d-44bc-e34d-01fe7d697f2b"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logger.info('ğŸ§® Setting up accuracy function...')\n",
        "    acc_func = DiceMetric(\n",
        "        include_background=True,\n",
        "        reduction=MetricReduction.MEAN_BATCH,\n",
        "    )\n",
        "    \n",
        "    i = 0\n",
        "    dice_scores = []\n",
        "    \n",
        "    logger.debug(f'ğŸ”¢ {i = }')\n",
        "    logger.debug(f'ğŸ“Š {dice_scores = }')\n",
        "\n",
        "    for val_data in val_loader:\n",
        "        logger.info('ğŸ“¥ Loading data...')\n",
        "        val_images = val_data['image']\n",
        "        logger.debug(f'ğŸ–¼ï¸ {val_images.shape = }')\n",
        "\n",
        "        logger.info('ğŸ“œ Loading affine matrix...')\n",
        "        if use_scan_list:\n",
        "            with open(json_path) as f:\n",
        "                d = json.load(f)\n",
        "            d['training'] = [entry for entry in d['training'] if entry['fold'] == fold]\n",
        "            affine_path = os.path.join(data_dir, d['training'][i]['image'][2])\n",
        "        else:\n",
        "            affine_path = os.path.join(data_dir, inference_cfg['input']['single_scan']['t1'])\n",
        "        affine = nib.load(affine_path).affine\n",
        "        logger.debug(f'ğŸ“ {affine.shape = }')\n",
        "\n",
        "        logger.info('ğŸ” Performing sliding window inference...')\n",
        "        val_outputs = sliding_window_inference(\n",
        "            val_images.to(device),\n",
        "            roi_size=roi_size,\n",
        "            sw_batch_size=4,\n",
        "            predictor=model,\n",
        "        )\n",
        "        logger.debug(f'ğŸ”¢ {val_outputs.shape = }')\n",
        "\n",
        "        logger.info('ğŸ”„ Applying post-processing...')\n",
        "        post_sigmoid = Activations(sigmoid=True)\n",
        "        post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
        "        val_outputs_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
        "        logger.debug(f'ğŸ”¢ {len(val_outputs_convert) = }')\n",
        "\n",
        "        if do_ground_truth:\n",
        "            logger.info('ğŸ“Š Calculating DICE scores...')\n",
        "            ground_truth = val_data['label']\n",
        "            acc_func.reset()\n",
        "            acc_func(y_pred=val_outputs_convert, y=ground_truth.to(device))\n",
        "            acc = acc_func.aggregate().cpu().numpy()\n",
        "            num_zeroes = [acc[0], acc[1], acc[2]].count(0.0)\n",
        "            mean = (acc[0] + acc[1] + acc[2]) / (3 - num_zeroes) if num_zeroes < 3 else 0  # Ignore cases with zero DICE\n",
        "            \n",
        "            logger.info(f'ğŸ“Š DICE (tc): {acc[0]}')\n",
        "            logger.info(f'ğŸ“Š DICE (wt): {acc[1]}')\n",
        "            logger.info(f'ğŸ“Š DICE (et): {acc[2]}')\n",
        "            logger.info(f'ğŸ“Š DICE (mean): {mean}')\n",
        "            dice_scores.append([acc[0], acc[1], acc[2], mean])\n",
        "\n",
        "        logger.info('ğŸ”„ Processing output...')\n",
        "        val_outputs = val_outputs.clone().cpu().numpy().squeeze()\n",
        "        logger.debug(f'ğŸ”¢ {val_outputs.shape = }')\n",
        "        segmentation_mask = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
        "        segmentation_mask = segmentation_mask[:-1]  # Discard the union\n",
        "        segmentation_mask = np.sum(segmentation_mask, axis=0)\n",
        "        for k, v in {1: 5, 3: 4, 2: 1, 5: 2}.items():  # Correctly assign labels. Use 5 as a temporary for swapping.\n",
        "            segmentation_mask[segmentation_mask == k] = v\n",
        "\n",
        "        logger.info('ğŸ·ï¸ Identifying unique labels and their counts...')\n",
        "        unique_labels, label_counts = np.unique(segmentation_mask, return_counts=True)\n",
        "        logger.debug(f'ğŸ”¢ {unique_labels = }')\n",
        "        logger.debug(f'ğŸ”¢ {label_counts = }')\n",
        "        for label, count in zip(unique_labels, label_counts):\n",
        "            logger.info(f'ğŸ·ï¸ Label {int(label)}: {count} voxels')\n",
        "        total_voxels = np.prod(segmentation_mask.shape)\n",
        "        logger.info(f'ğŸ“Š Total voxels: {total_voxels}')\n",
        "        for label, count in zip(unique_labels, label_counts):\n",
        "            percentage = (count / total_voxels) * 100\n",
        "            logger.info(f'ğŸ“Š Label {int(label)}: {percentage:.2f}% of total volume')\n",
        "\n",
        "        logger.info('ğŸ§  Creating NIfTI image...')\n",
        "        nii = nib.Nifti1Image(segmentation_mask.astype(np.uint8), affine=affine)\n",
        "\n",
        "        if use_scan_list:  # If inferring on multiple scans, add the T1 file path to the name so they can be easily distinguished.\n",
        "            dirs = affine_path.replace('/', '.')\n",
        "            dir, fname = os.path.split(inference_cfg['output']['file_path'])\n",
        "            save_pth = os.path.join(dir, f'{dirs}_{fname}')\n",
        "        else:\n",
        "            save_pth = inference_cfg['output']['file_path']\n",
        "            \n",
        "        logger.info('ğŸ“ Creating output directory...')\n",
        "        output_directory = os.path.dirname(save_pth)\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "        logger.info(f'ğŸ’¾ Saving as {save_pth}...')\n",
        "        nib.save(nii, save_pth)\n",
        "        i += 1\n",
        "\n",
        "if do_ground_truth:\n",
        "    logger.info('ğŸ“Š Calculating mean DICE scores...')\n",
        "    mask = np.ma.masked_equal(dice_scores, 0)  # Ignore cases with zero DICE\n",
        "    mean = mask.mean(axis=0).filled(np.nan)\n",
        "\n",
        "    logger.info('ğŸ“Š Mean DICE scores:')\n",
        "    logger.info(f'ğŸ“Š DICE (tc): {mean[0]}')\n",
        "    logger.info(f'ğŸ“Š DICE (wt): {mean[1]}')\n",
        "    logger.info(f'ğŸ“Š DICE (et): {mean[2]}')\n",
        "    logger.info(f'ğŸ“Š DICE (mean): {mean[3]}')\n",
        "\n",
        "logger.info('âœ… Inference complete.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
