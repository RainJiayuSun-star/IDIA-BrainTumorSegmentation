{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9O2FMEC9OeU"
      },
      "source": [
        "# Overview\n",
        "**BRAIN SEGMENTATION INFERENCE**\n",
        "This Jupyter notebook is designed to run inference on a brain MRI scan using a pre-trained segmentation model. It downloads a model from the cloud (requiring AWS access keys to be set up properly) and then runs inference on a single scan. The output is a nifti file with the segmentation labels. You may wish to then load it in a program such as Slicer to view the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjoOMwOTNJTd"
      },
      "source": [
        "# Setup\n",
        "First, set up a python environment with the necessary packages.\n",
        "\n",
        "For instance, in VSCode or Cursor, press Ctrl+Shift+P and type \"Python: Create Environment\" and follow the prompts.\n",
        "\n",
        "Or, on the command line, run:\n",
        "```bash\n",
        "python -m venv brain_segmentation_inference_env\n",
        "```\n",
        "--or--\n",
        "```bash\n",
        "python3 -m venv brain_segmentation_inference_env\n",
        "```\n",
        "and then either \n",
        "```bash\n",
        "brain_segmentation_inference_env\\Scripts\\activate\n",
        "```\n",
        "on Windows, or\n",
        "```bash\n",
        "source brain_segmentation_inference_env/bin/activate\n",
        "```\n",
        "on macOS and Linux.\n",
        "\n",
        "Then, install the necessary packages:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "After all that, this cell should run without error and import all the necessary packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Create a named logger\n",
        "logger = logging.getLogger('brain_segmentation')\n",
        "logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a console handler and set its level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create a formatter\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Add the formatter to the console handler\n",
        "console_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the console handler to the logger\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Turn off logs from all other loggers\n",
        "for name in logging.root.manager.loggerDict:\n",
        "    if name != 'brain_segmentation':\n",
        "        logging.getLogger(name).setLevel(logging.CRITICAL)\n",
        "\n",
        "logger.info('üöÄ Setting up logging...')\n",
        "logger.debug(f'üîß Current logging level: {logger.getEffectiveLevel()}')\n",
        "logger.info('‚úÖ Logging setup complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SdAAZeqmPNsv",
        "outputId": "2706f96f-1b8d-4f0d-9f3e-b0f3c8dbaa2f"
      },
      "outputs": [],
      "source": [
        "logger.info('üì¶ Importing outside packages...')\n",
        "import os\n",
        "import torch\n",
        "import json, yaml\n",
        "import tempfile\n",
        "import boto3\n",
        "import pprint\n",
        "from tqdm import tqdm\n",
        "from smart_open import open\n",
        "from monai.transforms import AsDiscrete, Activations\n",
        "from monai.utils.enums import MetricReduction\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.inferers import sliding_window_inference\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "logger.info('‚úÖ Outside packages imported.')\n",
        "\n",
        "logger.info('üì¶ Importing local packages...')\n",
        "from core_common import get_loader_val, datafold_read\n",
        "import custom_model\n",
        "logger.info('‚úÖ Local packages imported.')\n",
        "\n",
        "logger.info('üì¶ Setting up device...')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info('‚úÖ Device set up.')\n",
        "logger.debug(f'üñ•Ô∏è {device = }')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OeN_BW_NJTj"
      },
      "source": [
        "# Model\n",
        "This part loads the model from the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX5dkAr0NJTl"
      },
      "source": [
        "## Download\n",
        "This part downloads the model from the cloud.\n",
        "\n",
        "You'll need to have an AWS profile named 'theta-model-downloader'. You can create this profile by running this command in your terminal:\n",
        "```bash\n",
        "aws configure --profile theta-model-downloader\n",
        "```\n",
        "and entering your AWS credentials.\n",
        "\n",
        "If you do not have the AWS command line tools installed, you can install them by following the instructions [here](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb1qCskWNJTm",
        "outputId": "a8a36564-66cf-4cf1-c8e9-b7201c6f6b93"
      },
      "outputs": [],
      "source": [
        "logger.info('üìÇ Loading configuration file...')\n",
        "config_pth = 'config/config.yaml'\n",
        "\n",
        "logger.info('üîì Opening configuration file...')\n",
        "with open(config_pth, 'r') as file:\n",
        "    logger.info('üìñ Reading YAML content...')\n",
        "    inference_cfg = yaml.safe_load(file)\n",
        "logger.debug(f'üîß {inference_cfg = }')\n",
        "logger.info('‚úÖ Configuration loaded.')\n",
        "\n",
        "logger.info('üîß Defining utility function...')\n",
        "def check_optional_key(x: dict, key_name, true_val):\n",
        "    logger.debug(f'üîç Checking for key: {key_name} with value: {true_val}')\n",
        "    result = (key_name in x.keys()) and (x[key_name] == true_val)\n",
        "    logger.debug(f'üîç Result of check: {result}')\n",
        "    return result\n",
        "logger.info('‚úÖ Utility function defined.')\n",
        "\n",
        "logger.info('‚òÅÔ∏è Setting up AWS S3 connection...')\n",
        "bucket = inference_cfg['model']['bucket']\n",
        "key = inference_cfg['model']['key']\n",
        "logger.info('üîê Creating AWS session...')\n",
        "session = boto3.Session(\n",
        "    profile_name='theta-model-downloader',\n",
        "    region_name=inference_cfg['model']['region'])\n",
        "logger.info('üîó Creating S3 client...')\n",
        "s3_client = session.client('s3')\n",
        "logger.debug(f'ü™£ {bucket = }')\n",
        "logger.debug(f'üîë {key = }')\n",
        "logger.info('‚úÖ AWS S3 connection set up.')\n",
        "\n",
        "logger.info('üì¶ Fetching model metadata...')\n",
        "logger.info('üîç Retrieving object metadata from S3...')\n",
        "metadata = s3_client.head_object(Bucket=bucket, Key=key)\n",
        "file_size = metadata['ContentLength']\n",
        "logger.info('üîß Parsing model configuration from metadata...')\n",
        "model_cfg = yaml.safe_load(metadata['Metadata']['cfg'])\n",
        "logger.debug(f'üìè {file_size = }')\n",
        "logger.debug(f'üîß {model_cfg = }')\n",
        "logger.info('‚úÖ Model metadata fetched.')\n",
        "\n",
        "logger.info('‚¨áÔ∏è Downloading model checkpoint...')\n",
        "logger.info('üìÅ Creating temporary file...')\n",
        "with tempfile.NamedTemporaryFile(mode='wb', suffix='.ckpt', delete=False) as temp_file:\n",
        "    logger.info('üîÑ Setting up progress bar...')\n",
        "    with tqdm(total=file_size, unit='B', unit_scale=True, desc='Downloading checkpoint...') as progress_bar:\n",
        "        def update_progress(chunk):\n",
        "            progress_bar.update(chunk)\n",
        "        logger.info('üîÑ Starting file download...')\n",
        "        s3_client.download_fileobj(Bucket=bucket, Key=key, Fileobj=temp_file, Callback=update_progress)\n",
        "\n",
        "    logger.info('üíæ Flushing temporary file...')\n",
        "    temp_file.flush()\n",
        "    logger.info('üîÑ Loading checkpoint into memory...')\n",
        "    checkpoint = torch.load(temp_file.name, map_location=device)\n",
        "    logger.info('üóëÔ∏è Closing and removing temporary file...')\n",
        "    temp_file.close()\n",
        "    os.unlink(temp_file.name) \n",
        "logger.info('‚úÖ Model checkpoint downloaded and loaded.')\n",
        "\n",
        "logger.info('üîÑ Processing state dictionary...')\n",
        "state_dict = checkpoint['state_dict']\n",
        "logger.info('üîÑ Removing \"model.\" prefix from state dict keys...')\n",
        "state_dict = {key.replace('model.', ''): value for key, value in state_dict.items()}\n",
        "logger.debug(f'üîë State dict keys: {state_dict.keys()}')\n",
        "logger.info('‚úÖ State dictionary processed.')\n",
        "\n",
        "logger.info('üîÑ Printing model configuration...')\n",
        "pprint.pprint(model_cfg)\n",
        "logger.info('‚úÖ Model configuration printed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9E4uJj9NJTn"
      },
      "source": [
        "## Load\n",
        "Once the model has been downloaded, this section loads the model into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujk2UMcINJTn",
        "outputId": "e29fca60-dc4b-4df8-b32d-d9f147c95a75"
      },
      "outputs": [],
      "source": [
        "def listify_3d(x: dict):\n",
        "    logger.info('üßä Listifying 3D dimensions...')\n",
        "    dimensions = [x['h'], x['w'], x['d']]\n",
        "    logger.debug(f'üìè Listified dimensions: {dimensions = }')\n",
        "    return dimensions\n",
        "\n",
        "logger.info('üéõÔ∏è Fetching hyperparameters...')\n",
        "hparams = model_cfg['hyperparameter']\n",
        "logger.debug(f'üéõÔ∏è Hyperparameters: {hparams = }')\n",
        "\n",
        "logger.info('üîó Fetching label union...')\n",
        "union = model_cfg['data']['label_union']\n",
        "logger.debug(f'üîó Label union: {union = }')\n",
        "\n",
        "logger.info('üìê Calculating ROI size...')\n",
        "roi_size = listify_3d(hparams['roi'])\n",
        "logger.debug(f'üìê ROI size: {roi_size = }')\n",
        "\n",
        "logger.info('üèóÔ∏è Creating model instance...')\n",
        "model = custom_model.CustomSwinUNETR(\n",
        "    in_channels       = 4, # one per MRI modality: T1, T2, T1-Contrast, FLAIR\n",
        "    img_size          = roi_size,\n",
        "    out_channels      = 4 if union else 3, # one per label: tumor core, whole tumor, enhancing tumor\n",
        "    feature_size      = hparams['feature_size'],\n",
        "    use_checkpoint    = True,\n",
        "    depths            = hparams['depths'],\n",
        "    num_heads         = hparams['num_heads'],\n",
        "    norm_name         = hparams['norm_name'],\n",
        "    normalize         = hparams['normalize'],\n",
        "    downsample        = hparams['downsample'],\n",
        "    use_v2            = hparams['use_v2'],\n",
        "    mlp_ratio         = hparams['mlp_ratio'],\n",
        "    qkv_bias          = hparams['qkv_bias'],\n",
        "    patch_size        = hparams['patch_size'],\n",
        "    window_size       = hparams['window_size'],\n",
        ")\n",
        "logger.info('‚úÖ Model instance created.')\n",
        "\n",
        "logger.info('üîë Fetching first model state key...')\n",
        "first_model_state_key = next(iter(model.state_dict().keys()))\n",
        "logger.debug(f'‚úÖ First model state key: {first_model_state_key = }')\n",
        "\n",
        "logger.info('üîë Fetching first state dict key...')\n",
        "first_state_dict_key = next(iter(state_dict.keys()))\n",
        "logger.debug(f'‚úÖ First state dict key: {first_state_dict_key = }')\n",
        "\n",
        "logger.info('üíæ Loading state dictionary into model...')\n",
        "model.load_state_dict(state_dict)\n",
        "logger.info('‚úÖ State dictionary loaded.')\n",
        "\n",
        "logger.info('üñ•Ô∏è Moving model to device...')\n",
        "model.to(device)\n",
        "logger.info('‚úÖ Model moved to device.')\n",
        "\n",
        "logger.info('üß† Setting model to evaluation mode...')\n",
        "model.eval()\n",
        "logger.info('‚úÖ Model set to evaluation mode.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYlZMp0kNJTn"
      },
      "source": [
        "# Data\n",
        "This section loads the data defined in your `config/config.yaml` file. Please see the example config file for more details. It requires you to have a nifti file for each MRI modality, typically T1, T2, T1-Contrast, and FLAIR, and the location to store the output nifti segmentation file. There are also instructions for loading multiple scans from a json file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFiR4fUuNJTo",
        "outputId": "d75fb267-4a8b-4974-91b8-6fc54694707d"
      },
      "outputs": [],
      "source": [
        "logger.info('üî¢ Setting up fold and configuration...')\n",
        "fold = 1  # Let the validation fold be 1 - same convention as during training.\n",
        "logger.debug(f'üìä {fold = }')\n",
        "\n",
        "logger.info('üîç Checking optional configuration keys...')\n",
        "do_ground_truth: bool = check_optional_key(inference_cfg['input'], 'compute_dice', True)\n",
        "logger.debug(f'üéØ {do_ground_truth = }')\n",
        "use_scan_list: bool = check_optional_key(inference_cfg['input'], 'mode', 'multi')\n",
        "logger.debug(f'üìã {use_scan_list = }')\n",
        "\n",
        "data_dir = inference_cfg['input']['data_dir']\n",
        "logger.debug(f'üìÇ {data_dir = }')\n",
        "\n",
        "logger.info('üìÅ Loading scan data...')\n",
        "if use_scan_list:\n",
        "    logger.info('üìö Using multiple scans from JSON...')\n",
        "    json_path = inference_cfg['input']['multi_scan']['scan_list']\n",
        "    _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
        "    with open(json_path) as f:\n",
        "        test_instance = json.load(f)['training'][0]['image'][2]  # To get image size, grab the T1 scan.\n",
        "else:\n",
        "    logger.info('üñºÔ∏è Using single scan...')\n",
        "    test_instance = inference_cfg['input']['single_scan']['t1']  # To get image size, grab the T1 scan.\n",
        "    json_data = {\n",
        "        'training': [\n",
        "            {\n",
        "                'fold': fold,\n",
        "                'image': [\n",
        "                    inference_cfg['input']['single_scan']['flair'],\n",
        "                    inference_cfg['input']['single_scan']['t1c'],\n",
        "                    inference_cfg['input']['single_scan']['t1'],\n",
        "                    inference_cfg['input']['single_scan']['t2']\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    if do_ground_truth:\n",
        "        json_data['training'][0]['label'] = inference_cfg['input']['single_scan']['ground_truth']\n",
        "\n",
        "    logger.info('üìù Creating temporary JSON file...')\n",
        "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.json', delete=False) as temp_file:  # Use delete=False to fix a permissions error on Windows.\n",
        "        json.dump(json_data, temp_file, indent=4)\n",
        "        temp_file.flush()\n",
        "        json_path = temp_file.name\n",
        "        _, validation_files = datafold_read(datalist=json_path, basedir=data_dir, fold=fold)\n",
        "        temp_file.close()\n",
        "        os.unlink(temp_file.name)\n",
        "    logger.info('‚úÖ Temporary JSON file created and processed.')\n",
        "\n",
        "logger.info('üìè Loading image size...')\n",
        "resize_shape = list(nib.load(os.path.join(data_dir, test_instance)).shape)\n",
        "logger.debug(f'üìê Scan size: {resize_shape = }')\n",
        "\n",
        "logger.info('üîÑ Setting up validation data loader...')\n",
        "val_loader = get_loader_val(\n",
        "    batch_size=1,\n",
        "    files=validation_files,\n",
        "    val_resize=resize_shape,\n",
        "    union=union,\n",
        "    workers=1,\n",
        "    cache_dir='',\n",
        "    dataset_type='Dataset',\n",
        "    add_label=do_ground_truth,\n",
        ")\n",
        "logger.info('‚úÖ Validation data loader setup complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_etOfVK4NJTo"
      },
      "source": [
        "# Inference\n",
        "This section takes the loaded model and runs inference on loaded scans, and saves the output to the location specified in your `config/config.yaml` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH4JxkCUNJTo",
        "outputId": "c2c40d5c-e52d-44bc-e34d-01fe7d697f2b"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logger.info('üßÆ Setting up accuracy function...')\n",
        "    acc_func = DiceMetric(\n",
        "        include_background=True,\n",
        "        reduction=MetricReduction.MEAN_BATCH,\n",
        "    )\n",
        "    \n",
        "    i = 0\n",
        "    dice_scores = []\n",
        "    \n",
        "    logger.debug(f'üî¢ {i = }')\n",
        "    logger.debug(f'üìä {dice_scores = }')\n",
        "\n",
        "    for val_data in val_loader:\n",
        "        logger.info('üì• Loading data...')\n",
        "        val_images = val_data['image']\n",
        "        logger.debug(f'üñºÔ∏è {val_images.shape = }')\n",
        "\n",
        "        logger.info('üìú Loading affine matrix...')\n",
        "        if use_scan_list:\n",
        "            with open(json_path) as f:\n",
        "                d = json.load(f)\n",
        "            d['training'] = [entry for entry in d['training'] if entry['fold'] == fold]\n",
        "            affine_path = os.path.join(data_dir, d['training'][i]['image'][2])\n",
        "        else:\n",
        "            affine_path = os.path.join(data_dir, inference_cfg['input']['single_scan']['t1'])\n",
        "        affine = nib.load(affine_path).affine\n",
        "        logger.debug(f'üìê {affine.shape = }')\n",
        "\n",
        "        logger.info('üîç Performing sliding window inference...')\n",
        "        val_outputs = sliding_window_inference(\n",
        "            val_images.to(device),\n",
        "            roi_size=roi_size,\n",
        "            sw_batch_size=4,\n",
        "            predictor=model,\n",
        "        )\n",
        "        logger.debug(f'üî¢ {val_outputs.shape = }')\n",
        "\n",
        "        logger.info('üîÑ Applying post-processing...')\n",
        "        post_sigmoid = Activations(sigmoid=True)\n",
        "        post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
        "        val_outputs_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
        "        logger.debug(f'üî¢ {len(val_outputs_convert) = }')\n",
        "\n",
        "        if do_ground_truth:\n",
        "            logger.info('üìä Calculating DICE scores...')\n",
        "            ground_truth = val_data['label']\n",
        "            acc_func.reset()\n",
        "            acc_func(y_pred=val_outputs_convert, y=ground_truth.to(device))\n",
        "            acc = acc_func.aggregate().cpu().numpy()\n",
        "            num_zeroes = [acc[0], acc[1], acc[2]].count(0.0)\n",
        "            mean = (acc[0] + acc[1] + acc[2]) / (3 - num_zeroes) if num_zeroes < 3 else 0  # Ignore cases with zero DICE\n",
        "            \n",
        "            logger.info(f'üìä DICE (tc): {acc[0]}')\n",
        "            logger.info(f'üìä DICE (wt): {acc[1]}')\n",
        "            logger.info(f'üìä DICE (et): {acc[2]}')\n",
        "            logger.info(f'üìä DICE (mean): {mean}')\n",
        "            dice_scores.append([acc[0], acc[1], acc[2], mean])\n",
        "\n",
        "        logger.info('üîÑ Processing output...')\n",
        "        val_outputs = val_outputs.clone().cpu().numpy().squeeze()\n",
        "        logger.debug(f'üî¢ {val_outputs.shape = }')\n",
        "        segmentation_mask = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs]\n",
        "        segmentation_mask = segmentation_mask[:-1]  # Discard the union\n",
        "        segmentation_mask = np.sum(segmentation_mask, axis=0)\n",
        "        for k, v in {1: 5, 3: 4, 2: 1, 5: 2}.items():  # Correctly assign labels. Use 5 as a temporary for swapping.\n",
        "            segmentation_mask[segmentation_mask == k] = v\n",
        "\n",
        "        logger.info('üè∑Ô∏è Identifying unique labels and their counts...')\n",
        "        unique_labels, label_counts = np.unique(segmentation_mask, return_counts=True)\n",
        "        logger.debug(f'üî¢ {unique_labels = }')\n",
        "        logger.debug(f'üî¢ {label_counts = }')\n",
        "        for label, count in zip(unique_labels, label_counts):\n",
        "            logger.info(f'üè∑Ô∏è Label {int(label)}: {count} voxels')\n",
        "        total_voxels = np.prod(segmentation_mask.shape)\n",
        "        logger.info(f'üìä Total voxels: {total_voxels}')\n",
        "        for label, count in zip(unique_labels, label_counts):\n",
        "            percentage = (count / total_voxels) * 100\n",
        "            logger.info(f'üìä Label {int(label)}: {percentage:.2f}% of total volume')\n",
        "\n",
        "        logger.info('üß† Creating NIfTI image...')\n",
        "        nii = nib.Nifti1Image(segmentation_mask.astype(np.uint8), affine=affine)\n",
        "\n",
        "        if use_scan_list:  # If inferring on multiple scans, add the T1 file path to the name so they can be easily distinguished.\n",
        "            dirs = affine_path.replace('/', '.')\n",
        "            dir, fname = os.path.split(inference_cfg['output']['file_path'])\n",
        "            save_pth = os.path.join(dir, f'{dirs}_{fname}')\n",
        "        else:\n",
        "            save_pth = inference_cfg['output']['file_path']\n",
        "            \n",
        "        logger.info('üìÅ Creating output directory...')\n",
        "        output_directory = os.path.dirname(save_pth)\n",
        "        os.makedirs(output_directory, exist_ok=True)\n",
        "        logger.info(f'üíæ Saving as {save_pth}...')\n",
        "        nib.save(nii, save_pth)\n",
        "        i += 1\n",
        "\n",
        "if do_ground_truth:\n",
        "    logger.info('üìä Calculating mean DICE scores...')\n",
        "    mask = np.ma.masked_equal(dice_scores, 0)  # Ignore cases with zero DICE\n",
        "    mean = mask.mean(axis=0).filled(np.nan)\n",
        "\n",
        "    logger.info('üìä Mean DICE scores:')\n",
        "    logger.info(f'üìä DICE (tc): {mean[0]}')\n",
        "    logger.info(f'üìä DICE (wt): {mean[1]}')\n",
        "    logger.info(f'üìä DICE (et): {mean[2]}')\n",
        "    logger.info(f'üìä DICE (mean): {mean[3]}')\n",
        "\n",
        "logger.info('‚úÖ Inference complete.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
